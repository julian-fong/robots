{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 128\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "max_iters = 50000\n",
    "eval_iters = 200\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !'(),-.:?ABCDEFGHIJKLMNOPRSTUVWYabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "with open(os.getcwd()+'\\\\data\\\\albhed.txt', 'r', encoding='utf8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "# create a mapping from characters to integers\n",
    "s_to_i = { ch:i+1 for i,ch in enumerate(chars) }\n",
    "s_to_i[\"<PAD>\"] = 0\n",
    "i_to_s = { i:s for s,i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x: [s_to_i[i] for i in x]\n",
    "decode = lambda y: [i_to_s[i] for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    en_sen = ''\n",
    "    a_sen = ''\n",
    "    i = 0\n",
    "    en = True\n",
    "    a = False\n",
    "    while i < len(text):\n",
    "        if en:\n",
    "            if text[i] != '(':\n",
    "                en_sen = en_sen + text[i]\n",
    "            elif text[i] == '(':\n",
    "                en = False\n",
    "                a = True \n",
    "        if a:\n",
    "            if text[i] != ')' and text[i] != '(':\n",
    "                a_sen = a_sen + text[i]\n",
    "            elif text[i] == ')':\n",
    "                a = False     \n",
    "        i += 1\n",
    "    return a_sen.strip(), en_sen.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_of_words(word):\n",
    "    len_pad = block_size - len(word)\n",
    "    t = torch.tensor(encode([*word] + len_pad*[\"<PAD>\"])).unsqueeze(0)\n",
    "    assert t.size(dim=0) == 1 and t.size(dim=1) == 64, print(t.size())\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english_sentence = []\n",
    "# albhed_sentence = []\n",
    "\n",
    "english_sentence = torch.empty((0, 64), dtype = torch.long)\n",
    "albhed_sentence = torch.empty((0, 64), dtype = torch.long)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\albhed.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        line = \" \".join(line.split())\n",
    "        if line != '':\n",
    "            if ':' in line:\n",
    "                text = line.split(\":\")[1]\n",
    "                en, a = parse_text(text)\n",
    "                #english_sentence.append(en)\n",
    "                #albhed_sentence.append(a)\n",
    "\n",
    "                english_sentence = torch.cat([english_sentence, create_tensor_of_words(en)])\n",
    "                albhed_sentence = torch.cat([albhed_sentence, create_tensor_of_words(a)])\n",
    "            else:\n",
    "                en, a = parse_text(line)\n",
    "                # english_sentence.append(en)\n",
    "                # albhed_sentence.append(a)\n",
    "                english_sentence = torch.cat([english_sentence, create_tensor_of_words(en)])\n",
    "                albhed_sentence = torch.cat([albhed_sentence, create_tensor_of_words(a)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET AND DATALOADERS\n",
    "n = int(0.9*len(english_sentence))\n",
    "\n",
    "train_data_en = english_sentence[:n]\n",
    "val_data_en = albhed_sentence[n:]\n",
    "\n",
    "train_data_a = english_sentence[:n]\n",
    "val_data_a = albhed_sentence[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    datax = train_data_en if split == 'train' else val_data_en\n",
    "    datay = train_data_a if split == 'train' else val_data_a\n",
    "    ix = torch.randint(len(datax) - block_size, (batch_size,))\n",
    "    x = torch.stack([datax[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([datay[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    x, y = x.to(device), y.to(device), t.to(device)\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def PositionalEncoding(seq_len, n_embd):\n",
    "        \n",
    "    pos_enc = torch.zeros(seq_len, n_embd)\n",
    "    position = torch.arange(0, seq_len, dtype = torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd, 2) * (-math.log(10000.0) / n_embd))\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pos_enc.to(device)\n",
    "\n",
    "def get_padding_mask_matrix(x, x_embed):\n",
    "    \"\"\"\n",
    "    x is the (B, T) tokenized matrix with padding included\n",
    "    x_embed is the embedded matrix that we will convert all rows to zero based on the corresponding row index = padding index\n",
    "    \"\"\"\n",
    "\n",
    "    #locate every index in each tokenized sentence which contains the pad index\n",
    "    pad_indices = torch.nonzero(x == 100277).squeeze().to(device) #This will return a (N, 2) where the first column represents the sentence (batch_index) and the second column represents the corresponding index which is the pad index (This 2nd column represents which row we will set to all zeros)\n",
    "\n",
    "    #initialize a torch.ones of the shape of the embedding matrix\n",
    "    mask = torch.ones(x_embed.shape).to(device)\n",
    "\n",
    "    #For each row in the pad_indices matrix, we go to pad_indices[0] to grab the current batch example, and we go to the corresponding row of the batch example using the value of pad_indices[1]. We turn every column of that row into zeros\n",
    "    #ex: if the current row is [1, 4], then we go to the 2nd batch example, go to the 4th row, and wipe it clean with zeroes\n",
    "    mask[pad_indices[:,0], pad_indices[:,1], :] = 0\n",
    "\n",
    "    #element-wise product\n",
    "    x_padded = x_embed * mask\n",
    "\n",
    "    return x_padded.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, decoder = False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        if self.decoder:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        #assume input is of size (B, T, C)\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        if self.decoder:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class crossHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out):\n",
    "        B, T, C = x.shape\n",
    "        #assume x is of shape (B, T, C)\n",
    "        #assume enc_out is of shape (B, T, C)\n",
    "\n",
    "        K = self.Wk(enc_out) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(enc_out) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "        # print(\"attn\",attention_scores.shape)\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, decoder):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, decoder) for _ in range(n_heads)])\n",
    "        #output of heads is of size (B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(head_size * n_heads, n_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([crossHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        x = torch.cat([h(x, enc_out) for h in self.heads], dim = -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed, padding_idx = 0, device = device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T)\n",
    "        return self.embedding(x.long()) * n_embed**(1/2)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4*n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = False)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x = self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderCrossBlock(nn.Module):\n",
    "    #one implementation of the multi head cross attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.heads = MultiHeadCrossAttention(head_size)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, parameters):\n",
    "        #assume parameters[0] is input of shape (B, T, C), It is the output of the decoder self attention layer\n",
    "        #assume parameters is a list of length 2: first element is the output of the previous hidden layer, and the 2nd element is the output of the encoder\n",
    "        \n",
    "        x = parameters[0]\n",
    "        enc_out = parameters[1]\n",
    "        \n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.heads(x, enc_out) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return [x, enc_out]\n",
    "\n",
    "class DecoderSelfBlock(nn.Module):\n",
    "    #one implementation of the multi head self attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_heads\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = True)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix_x = Embedding()\n",
    "        self.tok_embedding_matrix_y = Embedding()\n",
    "\n",
    "        #positional embedding is a function that requires no backpropagation, so we don't need to initialize it in here\n",
    "        \n",
    "        self.EncoderBlocks = nn.Sequential(*[EncoderBlock() for _ in range(n_layers)])\n",
    "        self.DecoderSelfBlocks = nn.Sequential(*[DecoderSelfBlock() for _ in range(n_layers)])\n",
    "        self.DecoderCrossBlocks = nn.Sequential(*[DecoderCrossBlock() for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
    "        self.final_linear = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    #looks to be very broken\n",
    "    def translate(self, x):\n",
    "        pass\n",
    "            \n",
    "    def forward(self, x, y, targets = None):\n",
    "        Bx, Tx = x.shape\n",
    "        Cx = n_embed\n",
    "        \n",
    "        tok_embed_x = self.tok_embedding_matrix_x(x)\n",
    "        pos_embed_x = PositionalEncoding(Tx, Cx)\n",
    "\n",
    "        By, Ty, = y.shape\n",
    "        Cy = n_embed\n",
    "\n",
    "        tok_embed_y = self.tok_embedding_matrix_y(y)\n",
    "        pos_embed_y = PositionalEncoding(Ty, Cy)\n",
    "\n",
    "        tok_pos_embed_x = tok_embed_x + pos_embed_x\n",
    "        tok_pos_embed_y = tok_embed_y + pos_embed_y\n",
    "        \n",
    "        masked_tok_embed_x = get_padding_mask_matrix(x, tok_pos_embed_x)\n",
    "        masked_tok_embed_y = get_padding_mask_matrix(y, tok_pos_embed_y)\n",
    "\n",
    "        x = masked_tok_embed_x\n",
    "        y = masked_tok_embed_y\n",
    "\n",
    "        #encoder\n",
    "        enc_out = self.EncoderBlocks(x)\n",
    "\n",
    "        #decoder self\n",
    "        y = self.DecoderSelfBlocks(y)\n",
    "        #decoder cross\n",
    "        #its ideal to send in one parameter only (i.e self, x) when passing parameters through stacked layers in an nn.Sequential, so we have to combine our previous hidden state output along with the enc_out into one object\n",
    "        y = self.DecoderCrossBlocks([y, enc_out])\n",
    "\n",
    "        #grab the transformed decoder input from the cross attention layer\n",
    "        y = y[0]\n",
    "        #remaining layers\n",
    "        y = self.final_layernorm(y)\n",
    "        logits = self.final_linear(y)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(By*Ty, -1)\n",
    "            targets = targets.view(targets.shape[0]*targets.shape[1])\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.0003, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617]}]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = 100277)\n",
    "\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb, tb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "\n",
    "            B, T, = yb.shape\n",
    "            C = n_embed\n",
    "\n",
    "            logits = logits.view(B*T, -1)\n",
    "            tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "            loss = F.cross_entropy(logits, tb)\n",
    "            \n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb, tb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    B, T, = yb.shape\n",
    "    C = n_embed\n",
    "\n",
    "    logits = logits.view(B*T, -1)\n",
    "    tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "    loss = F.cross_entropy(logits, tb)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, tb = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whoa! It moves!<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(xb.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fruy! Ed sujac!<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(yb.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ruy! Ed sujac!<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(tb.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = AlBhedDataset([train_data_en, train_data_a])\n",
    "# valset = AlBhedDataset([val_data_en, val_data_a])\n",
    "\n",
    "# train_dataloader = DataLoader(trainset, batch_size = 128, shuffle = True)\n",
    "# val_dataloader = DataLoader(valset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for i in epoch:\n",
    "    if epoch % eval_interval == 0 or i == epoch - 1:\n",
    "        losses = estimate_loss()\n",
    "        lr_ = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr_}\")\n",
    "\n",
    "    for count, value in enumerate(train_dataloader):\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb, tb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        B, T, = yb.shape\n",
    "        C = n_embed\n",
    "\n",
    "        logits = logits.view(B*T, -1)\n",
    "        tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "        loss = F.cross_entropy(logits, tb)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "class AlBhedDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data_en = data[0]\n",
    "        self.data_a = data[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.dict = {}\n",
    "        input = self.data_en[idx]\n",
    "        output = self.data_a[idx]\n",
    "\n",
    "        t = self.data_a[:, 1:]\n",
    "        #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "        t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "        target = t\n",
    "\n",
    "        self.dict[\"input\"] = input\n",
    "        self.dict[\"output\"] = output\n",
    "        self.dict[\"target\"] = target\n",
    "\n",
    "        return self.dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
