{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100282\n",
      "{'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276, '<|PAD|>': 0, '<|START|>': 100278, '<|END|>': 100279, '<|DEL|>': 100280, '!': 100281}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#dataset https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "#tiktoken api https://github.com/openai/tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|PAD|>\": 0,\n",
    "        \"<|START|>\": 100278,\n",
    "        \"<|END|>\": 100279,\n",
    "        \"<|DEL|>\": 100280,\n",
    "        \"!\": 100281\n",
    "    }\n",
    ")\n",
    "print(tokenizer.n_vocab) #this is the number of tokens in our tokenizer\n",
    "print(tokenizer._special_tokens) #prints out our special tokens \n",
    "\n",
    "specials = {\"<|PAD|>\",\"<|START|>\",\"<|END|>\", \"<|DEL|>\", \"!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 512\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    num_examples = 100000\n",
    "\n",
    "    en_max = 0\n",
    "    en_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_en = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            en_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > en_max:\n",
    "                en_max = len(tok_sentence)\n",
    "                print(en_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_en.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_en.append(block_size*[100280])\n",
    "\n",
    "    en_length = torch.tensor(en_length).float()     \n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0\n",
    "    de_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_de = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            de_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > de_max:\n",
    "                de_max = len(tok_sentence)\n",
    "                print(de_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_de.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_de.append(block_size*[100280])\n",
    "\n",
    "    de_length = torch.tensor(de_length).float()               \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "\n",
    "    #combine the arrays together\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    #check for indices in both sentences that have rows containing the DEL token\n",
    "    idx = np.where(sentences == 100280)\n",
    "\n",
    "    #delete every row that contains the DEL token\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = torch.tensor(sentences[0], dtype=torch.long)\n",
    "    sentences_de = torch.tensor(sentences[1], dtype=torch.long)\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(f\"Average length of english tokenized sentence: {torch.mean(en_length)}, and with std: {torch.std(en_length)}\")\n",
    "    print(f\"Average length of german tokenized sentence: {torch.mean(de_length)}, and with std: {torch.std(de_length)}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943e992a590e4bb480424f71118fe730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "50\n",
      "60\n",
      "67\n",
      "68\n",
      "82\n",
      "95\n",
      "122\n",
      "125\n",
      "132\n",
      "141\n",
      "154\n",
      "177\n",
      "191\n",
      "203\n",
      "295\n",
      "295\n",
      "Length of sentences: 100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaf58d05f244446a09352aebd273fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "73\n",
      "83\n",
      "91\n",
      "105\n",
      "120\n",
      "130\n",
      "135\n",
      "161\n",
      "163\n",
      "187\n",
      "239\n",
      "267\n",
      "284\n",
      "284\n",
      "Length of sentences: 100000\n",
      "Removing sentences whos length is greater than our block_size\n",
      "Length of new english sentences: 83474\n",
      "Length of new german sentences: 83474\n",
      "Average length of english tokenized sentence: 32.97871017456055, and with std: 17.489957809448242\n",
      "Average length of german tokenized sentence: 42.1722297668457, and with std: 22.630157470703125\n"
     ]
    }
   ],
   "source": [
    "create = True\n",
    "if create:\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND VAL DATASETS\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'rb') as f:\n",
    "    english_sentences = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'rb') as f:\n",
    "    german_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100280 in english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  1472, 18217,  9507,   387, 14792,  1268,  4228,  3489,   847,\n",
       "          706,  1903,   433,   311, 15405,   279,  2539,  4754,   315,   682,\n",
       "          701,  2955,  3241,   662,   220,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    y = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    y = F.pad(input = y, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch's positional encoding https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            x: (T, B, C)\n",
    "            We have to change our shape dimensions in to (T, B, C) and then change it back to (B, T, C) when done\n",
    "\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, decoder = False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size)\n",
    "        self.Wq = nn.Linear(n_embed, head_size)\n",
    "        self.Wv = nn.Linear(n_embed, head_size)\n",
    "\n",
    "        if self.decoder:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input is of size (B, T, C)\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        if self.decoder:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class crossHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size)\n",
    "        self.Wq = nn.Linear(n_embed, head_size)\n",
    "        self.Wv = nn.Linear(n_embed, head_size)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        #assume enc_out is of shape (B, T, C)\n",
    "\n",
    "        K = self.Wk(enc_out) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(enc_out) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, decoder):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, decoder) for _ in range(n_heads)])\n",
    "        #output of heads is of size (B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(head_size * n_heads, n_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([crossHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        x = torch.cat([h(x, enc_out) for h in self.heads], dim = -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4*n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = False)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x = self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderCrossBlock(nn.Module):\n",
    "    #one implementation of the multi head cross attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.heads = MultiHeadCrossAttention(head_size)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        #assume x is input of shape (B, T, C), x is the output of the decoder self attention layer\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.heads(x, enc_out) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderSelfBlock(nn.Module):\n",
    "    #one implementation of the multi head self attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_heads\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = True)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix_x = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_x = PositionalEncoding(n_embd, dropout = dropout)\n",
    "\n",
    "        self.tok_embedding_matrix_y = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_y = PositionalEncoding(n_embd, dropout = dropout)\n",
    "\n",
    "        self.EncoderBlocks = nn.Sequential(*[EncoderBlock() for _ in range(n_layers)])\n",
    "        self.DecoderSelfBlocks = nn.Sequential(*[DecoderSelfBlock() for _ in range(n_layers)])\n",
    "        self.DecoderCrossBlocks = nn.Sequential(*[DecoderCrossBlock() for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        tok_embed_x = self.tok_embedding_matrix_x(input)\n",
    "        pos_embed_x = self.pos_embedding_x(token_embed_x.view(T,B,C)).view(B, T, C)\n",
    "\n",
    "        tok_embed_y = self.tok_embedding_matrix_y(input)\n",
    "        pos_embed_y = self.pos_embedding_y(token_embed_y.view(T,B,C)).view(B, T, C)\n",
    "\n",
    "        x = tok_embed_x + pos_embed_x\n",
    "        y = tok_embed_y + pos_embed_y\n",
    "\n",
    "        #encoder\n",
    "        enc_out = self.EncoderBlocks(x)\n",
    "\n",
    "        #decoder self\n",
    "        y = self.DecoderSelfBlocks(y)\n",
    "        \n",
    "        #decoder cross\n",
    "        y = self.CrossAttentionBlocks(y, enc_out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17123, 63621, 52424, 36214, 10675, 67899, 46588, 73345,  7587, 18486,\n",
      "        63459, 17373,  1692, 56649,   482, 37312])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, n_embed)\n",
    "pos_embedding = PositionalEncoding(n_embed, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embed_x = tok_embedding_matrix(xb) #(B, T, C)\n",
    "B, T, C = token_embed_x.shape\n",
    "pos_embed_x = pos_embedding(token_embed_x.view(T,B,C)).view(B, T, C) #(B, T, C)\n",
    "\n",
    "input = token_embed_x + pos_embed_x #(B, T, C)\n",
    "\n",
    "token_embed_y = tok_embedding_matrix(yb) #(B, T, C)\n",
    "B, T, C = token_embed_y.shape\n",
    "pos_embed_y = pos_embedding(token_embed_y.view(T,B,C)).view(B, T, C) #(B, T, C)\n",
    "\n",
    "target = token_embed_y + pos_embed_y #(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 512])"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Head(head_size = 64, decoder = False)\n",
    "e = MultiHeadSelfAttention(64, decoder = False)\n",
    "e = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "e(input).shape\n",
    "\n",
    "out_e = e(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Head(head_size = 64, decoder = True)\n",
    "d = MultiHeadSelfAttention(head_size = 64, decoder = True)\n",
    "d = DecoderSelfBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "d(target).shape\n",
    "\n",
    "out_d = d(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = crossHead(head_size = 64)\n",
    "c = MultiHeadCrossAttention(head_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5961e-01, -0.0000e+00,  2.3434e-02,  ...,  9.1408e-02,\n",
       "           1.2809e-02, -1.3358e-02],\n",
       "         [-5.2937e-02, -0.0000e+00,  6.2386e-02,  ..., -0.0000e+00,\n",
       "           6.4907e-03,  6.2555e-02],\n",
       "         [ 2.4392e-02, -3.1080e-02,  8.6604e-02,  ..., -1.2029e-01,\n",
       "           6.8772e-03,  8.7691e-02],\n",
       "         ...,\n",
       "         [-5.8023e-03, -1.3832e-01,  4.6914e-02,  ...,  6.0742e-02,\n",
       "          -5.9813e-02, -2.8790e-02],\n",
       "         [-6.2929e-03, -1.3868e-01,  4.7524e-02,  ...,  0.0000e+00,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [-9.3312e-03, -1.5363e-01,  0.0000e+00,  ...,  7.3021e-02,\n",
       "          -0.0000e+00, -3.3025e-02]],\n",
       "\n",
       "        [[-5.6573e-02, -3.2562e-01,  1.0243e-01,  ...,  2.4677e-01,\n",
       "          -0.0000e+00, -8.8965e-02],\n",
       "         [-6.4727e-02,  5.4738e-02,  1.4060e-01,  ...,  6.0478e-02,\n",
       "          -7.0306e-02, -0.0000e+00],\n",
       "         [-2.2432e-03,  2.9734e-02,  1.2461e-01,  ..., -4.5891e-03,\n",
       "          -0.0000e+00, -1.2388e-01],\n",
       "         ...,\n",
       "         [ 2.2815e-02, -1.5171e-01,  6.8049e-02,  ...,  9.2899e-02,\n",
       "          -5.2139e-02, -4.1005e-02],\n",
       "         [ 0.0000e+00, -1.4157e-01,  0.0000e+00,  ...,  7.8945e-02,\n",
       "          -5.2939e-02, -4.0564e-02],\n",
       "         [ 1.4369e-02, -1.6058e-01,  6.7011e-02,  ...,  9.0057e-02,\n",
       "          -0.0000e+00, -4.8027e-02]],\n",
       "\n",
       "        [[ 0.0000e+00, -2.3879e-01,  2.3882e-02,  ...,  2.0604e-01,\n",
       "          -0.0000e+00, -1.0541e-01],\n",
       "         [-0.0000e+00,  1.4661e-02,  2.1154e-02,  ...,  8.9277e-02,\n",
       "          -0.0000e+00, -0.0000e+00],\n",
       "         [ 5.7502e-02, -6.3754e-02, -3.2315e-04,  ...,  0.0000e+00,\n",
       "          -9.2196e-02, -2.5385e-02],\n",
       "         ...,\n",
       "         [ 7.1758e-03, -1.1658e-01,  0.0000e+00,  ...,  8.0334e-02,\n",
       "          -0.0000e+00,  1.1881e-02],\n",
       "         [ 0.0000e+00, -1.3539e-01,  3.3796e-02,  ...,  0.0000e+00,\n",
       "          -4.3525e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -0.0000e+00,  3.0525e-02,  ...,  0.0000e+00,\n",
       "          -5.6320e-02,  1.7040e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.6221e-04, -2.7202e-01,  1.3847e-02,  ...,  7.5098e-02,\n",
       "          -4.9026e-02, -0.0000e+00],\n",
       "         [-0.0000e+00, -1.3866e-01, -1.0845e-02,  ...,  0.0000e+00,\n",
       "          -8.4722e-02, -9.5408e-02],\n",
       "         [-2.0230e-02, -1.5253e-02, -4.9577e-02,  ...,  1.9869e-01,\n",
       "          -4.8064e-02, -1.8645e-02],\n",
       "         ...,\n",
       "         [ 0.0000e+00, -5.8233e-02,  4.2086e-02,  ...,  4.8902e-02,\n",
       "          -0.0000e+00,  0.0000e+00],\n",
       "         [ 6.4165e-02, -5.8813e-02,  3.8924e-02,  ...,  0.0000e+00,\n",
       "          -9.1250e-02,  4.7044e-03],\n",
       "         [ 5.0331e-02, -0.0000e+00,  5.2412e-02,  ...,  5.1893e-02,\n",
       "          -9.9684e-02,  2.7084e-02]],\n",
       "\n",
       "        [[ 1.5002e-01, -1.7140e-01,  6.0655e-02,  ...,  2.2957e-01,\n",
       "          -6.7823e-02, -5.1471e-02],\n",
       "         [ 2.5054e-02, -7.7871e-02,  1.5157e-02,  ..., -6.1144e-02,\n",
       "          -4.9518e-02, -4.9308e-02],\n",
       "         [-2.3130e-02, -0.0000e+00,  1.5358e-02,  ...,  3.2654e-02,\n",
       "          -4.2010e-02, -0.0000e+00],\n",
       "         ...,\n",
       "         [ 4.0160e-02, -7.0892e-02,  5.5885e-02,  ...,  4.7502e-02,\n",
       "          -0.0000e+00, -1.9393e-02],\n",
       "         [ 3.6258e-02, -7.0083e-02,  6.2235e-02,  ...,  6.1223e-02,\n",
       "          -5.7666e-02, -2.3323e-02],\n",
       "         [ 3.3457e-02, -0.0000e+00,  6.0987e-02,  ...,  4.7398e-02,\n",
       "          -6.1375e-02, -0.0000e+00]],\n",
       "\n",
       "        [[ 8.9806e-02, -2.7991e-01,  1.5097e-01,  ...,  9.8068e-02,\n",
       "           2.8939e-02, -5.2618e-02],\n",
       "         [ 1.2487e-01, -0.0000e+00, -3.1685e-03,  ...,  2.3770e-02,\n",
       "           1.4948e-01, -0.0000e+00],\n",
       "         [ 9.2633e-02, -1.0634e-01,  1.1683e-01,  ...,  7.6119e-03,\n",
       "           8.1698e-02, -1.6262e-01],\n",
       "         ...,\n",
       "         [ 2.4109e-02, -0.0000e+00,  0.0000e+00,  ...,  9.9927e-02,\n",
       "          -0.0000e+00, -3.7547e-02],\n",
       "         [ 2.2792e-02, -1.0422e-01,  4.6641e-02,  ...,  0.0000e+00,\n",
       "          -4.1486e-02, -0.0000e+00],\n",
       "         [ 1.7611e-02, -9.8280e-02,  3.2456e-02,  ...,  0.0000e+00,\n",
       "          -4.5120e-02, -1.0967e-02]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(out_d, out_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|PAD|>\", allowed_special = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ee0b04acb4cacacf37d05d30ef1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|PAD|> iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . <|PAD|>\n",
      "[0, 11245, 24532, 374, 264, 5644, 369, 1005, 25982, 902, 374, 17551, 439, 264, 1488, 1169, 555, 2231, 1919, 22145, 477, 14654, 304, 279, 51370, 13116, 320, 24359, 883, 315, 279, 9699, 6892, 354, 51370, 662, 220, 0]\n",
      "38\n",
      "38\n",
      "<|PAD|> iron cement protects the ingot against the hot , abrasive steel casting process . <|PAD|>\n",
      "[0, 11245, 24532, 36236, 279, 6892, 354, 2403, 279, 4106, 1174, 94804, 9699, 25146, 1920, 662, 220, 0]\n",
      "18\n",
      "<|PAD|> a fire restant repair cement for fire places , ovens , open fireplaces etc . <|PAD|>\n",
      "[0, 264, 4027, 2800, 519, 13023, 24532, 369, 4027, 7634, 1174, 297, 21778, 1174, 1825, 4027, 27170, 5099, 662, 220, 0]\n",
      "21\n",
      "<|PAD|> Construction and repair of highways and ... <|PAD|>\n",
      "[0, 24987, 323, 13023, 315, 60395, 323, 2564, 220, 0]\n",
      "10\n",
      "<|PAD|> An announcement must be commercial character . <|PAD|>\n",
      "[0, 1556, 17480, 2011, 387, 8518, 3752, 662, 220, 0]\n",
      "10\n",
      "38\n",
      "Length of sentences: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06345316f748eebf7f5a2b06fc3402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "Length of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "block_size = 10\n",
    "num_examples = 5\n",
    "\n",
    "en_max = 0 \n",
    "with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "    idx_en = []\n",
    "    sentences_en = []\n",
    "    for i in tqdm(range(num_examples)):\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "        print(sentence)\n",
    "        print(tokenizer.encode(sentence, allowed_special = specials))\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        print(len(tok_sentence))\n",
    "        if len(tok_sentence) > en_max:\n",
    "            en_max = len(tok_sentence)\n",
    "            print(en_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[100277]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            #idx_en.append(i)\n",
    "            sentences_en.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_en.append(block_size*[100280])\n",
    "\n",
    "print(en_max)    \n",
    "print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "\n",
    "de_max = 0 \n",
    "with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "    idx_de = []\n",
    "    sentences_de = []\n",
    "    for i in tqdm(range(num_examples)):\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        if len(tok_sentence) > de_max:\n",
    "            de_max = len(tok_sentence)\n",
    "            print(de_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[100277]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            #idx_en.append(i)\n",
    "            sentences_de.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_de.append(block_size*[100280])\n",
    "            \n",
    "print(de_max)  \n",
    "print(f\"Length of sentences: {len(sentences_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sentences_en).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array([sentences_en, sentences_de])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4\n",
      " 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "sentences = np.array([sentences_en, sentences_de])\n",
    "\n",
    "idx = np.where(sentences == 100280)\n",
    "print(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.delete(sentences, idx[1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(2, 0, 10), dtype=int32)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
