{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 256 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 512\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100282\n",
      "{'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276, '<|PAD|>': 0, '<|START|>': 100278, '<|END|>': 100279, '<|DEL|>': 100280, '!': 100281}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#dataset https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "#tiktoken api https://github.com/openai/tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|PAD|>\": 0,\n",
    "        \"<|START|>\": 100278,\n",
    "        \"<|END|>\": 100279,\n",
    "        \"<|DEL|>\": 100280,\n",
    "        \"!\": 100281\n",
    "    }\n",
    ")\n",
    "print(tokenizer.n_vocab) #this is the number of tokens in our tokenizer\n",
    "print(tokenizer._special_tokens) #prints out our special tokens \n",
    "\n",
    "specials = {\"<|PAD|>\",\"<|START|>\",\"<|END|>\", \"<|DEL|>\", \"!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    num_examples = 100000\n",
    "\n",
    "    en_max = 0 \n",
    "    with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "        idx_en = []\n",
    "        sentences_en = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            if len(tok_sentence) > en_max:\n",
    "                en_max = len(tok_sentence)\n",
    "                print(en_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_en.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_en.append(block_size*[100280])\n",
    "\n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0 \n",
    "    with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "        idx_de = []\n",
    "        sentences_de = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            if len(tok_sentence) > de_max:\n",
    "                de_max = len(tok_sentence)\n",
    "                print(de_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_de.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_de.append(block_size*[100280])\n",
    "                \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "\n",
    "    #combine the arrays together\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    #check for indices in both sentences that have rows containing the DEL token\n",
    "    idx = np.where(sentences == 100280)\n",
    "\n",
    "    #delete every row that contains the DEL token\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = torch.tensor(sentences[0], dtype=torch.long)\n",
    "    sentences_de = torch.tensor(sentences[1], dtype=torch.long)\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0c1401ad714792a4ba37b0d6e48486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "50\n",
      "60\n",
      "67\n",
      "68\n",
      "82\n",
      "95\n",
      "122\n",
      "125\n",
      "132\n",
      "141\n",
      "154\n",
      "177\n",
      "191\n",
      "203\n",
      "295\n",
      "295\n",
      "Length of sentences: 100000\n",
      "295\n",
      "Length of sentences: 100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a398e23851bf4d2c9a9161e4e7ba730f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "73\n",
      "83\n",
      "91\n",
      "105\n",
      "120\n",
      "130\n",
      "135\n",
      "161\n",
      "163\n",
      "187\n",
      "239\n",
      "267\n",
      "284\n",
      "284\n",
      "Length of sentences: 100000\n",
      "Removing sentences whos length is greater than our block_size\n",
      "Length of new english sentences: 99996\n",
      "Length of new german sentences: 99996\n"
     ]
    }
   ],
   "source": [
    "create = True\n",
    "if create:\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND VAL DATASETS\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'rb') as f:\n",
    "    english_sentences = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'rb') as f:\n",
    "    german_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100280 in english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    y = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    y = F.pad(input = yt, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "#xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch's positional encoding https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            x: (T, B, C)\n",
    "            We have to change our shape dimensions in to (T, B, C) and then change it back to (B, T, C) when done\n",
    "\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, decoder = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Wk = nn.Linear(n_embed, head_size)\n",
    "        self.Wq = nn.Linear(n_embed, head_size)\n",
    "        self.Wv = nn.Linear(n_embed, head_size)\n",
    "\n",
    "        if decoder:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input is of size (B, T, C)\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        if decoder:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, decoder):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, decoder) for _ in range(n_heads)])\n",
    "        #output of heads is of size (B, T, num_heads*head_size)\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "            out = proj(out)\n",
    "            out = self.dropout(out)\n",
    "\n",
    "            return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4*n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = False)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "        x = layernorm1(x) #B, T, C\n",
    "        x = x + self.sa(x) #B, T, C\n",
    "        x = layernorm2(x) #B, T, C\n",
    "        x = x = ffw(x) #B, T, C\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|PAD|>\", allowed_special = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ee0b04acb4cacacf37d05d30ef1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|PAD|> iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . <|PAD|>\n",
      "[0, 11245, 24532, 374, 264, 5644, 369, 1005, 25982, 902, 374, 17551, 439, 264, 1488, 1169, 555, 2231, 1919, 22145, 477, 14654, 304, 279, 51370, 13116, 320, 24359, 883, 315, 279, 9699, 6892, 354, 51370, 662, 220, 0]\n",
      "38\n",
      "38\n",
      "<|PAD|> iron cement protects the ingot against the hot , abrasive steel casting process . <|PAD|>\n",
      "[0, 11245, 24532, 36236, 279, 6892, 354, 2403, 279, 4106, 1174, 94804, 9699, 25146, 1920, 662, 220, 0]\n",
      "18\n",
      "<|PAD|> a fire restant repair cement for fire places , ovens , open fireplaces etc . <|PAD|>\n",
      "[0, 264, 4027, 2800, 519, 13023, 24532, 369, 4027, 7634, 1174, 297, 21778, 1174, 1825, 4027, 27170, 5099, 662, 220, 0]\n",
      "21\n",
      "<|PAD|> Construction and repair of highways and ... <|PAD|>\n",
      "[0, 24987, 323, 13023, 315, 60395, 323, 2564, 220, 0]\n",
      "10\n",
      "<|PAD|> An announcement must be commercial character . <|PAD|>\n",
      "[0, 1556, 17480, 2011, 387, 8518, 3752, 662, 220, 0]\n",
      "10\n",
      "38\n",
      "Length of sentences: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06345316f748eebf7f5a2b06fc3402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "Length of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "block_size = 10\n",
    "num_examples = 5\n",
    "\n",
    "en_max = 0 \n",
    "with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "    idx_en = []\n",
    "    sentences_en = []\n",
    "    for i in tqdm(range(num_examples)):\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "        print(sentence)\n",
    "        print(tokenizer.encode(sentence, allowed_special = specials))\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        print(len(tok_sentence))\n",
    "        if len(tok_sentence) > en_max:\n",
    "            en_max = len(tok_sentence)\n",
    "            print(en_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[100277]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            #idx_en.append(i)\n",
    "            sentences_en.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_en.append(block_size*[100280])\n",
    "\n",
    "print(en_max)    \n",
    "print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "\n",
    "de_max = 0 \n",
    "with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "    idx_de = []\n",
    "    sentences_de = []\n",
    "    for i in tqdm(range(num_examples)):\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        if len(tok_sentence) > de_max:\n",
    "            de_max = len(tok_sentence)\n",
    "            print(de_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[100277]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            #idx_en.append(i)\n",
    "            sentences_de.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_de.append(block_size*[100280])\n",
    "            \n",
    "print(de_max)  \n",
    "print(f\"Length of sentences: {len(sentences_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sentences_en).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array([sentences_en, sentences_de])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4\n",
      " 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "sentences = np.array([sentences_en, sentences_de])\n",
    "\n",
    "idx = np.where(sentences == 100280)\n",
    "print(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.delete(sentences, idx[1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(2, 0, 10), dtype=int32)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
