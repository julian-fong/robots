{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100281\n",
      "{'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276, '<|PAD|>': 100277, '<|START|>': 100278, '<|END|>': 100279, '<|DEL|>': 100280}\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_core_bpe', '_encode_bytes', '_encode_only_native_bpe', '_encode_single_piece', '_mergeable_ranks', '_pat_str', '_special_tokens', 'decode', 'decode_batch', 'decode_bytes', 'decode_bytes_batch', 'decode_single_token_bytes', 'decode_tokens_bytes', 'encode', 'encode_batch', 'encode_ordinary', 'encode_ordinary_batch', 'encode_single_token', 'encode_with_unstable', 'eot_token', 'max_token_value', 'n_vocab', 'name', 'special_tokens_set', 'token_byte_values']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#dataset https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "#tiktoken api https://github.com/openai/tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|PAD|>\": 100277,\n",
    "        \"<|START|>\": 100278,\n",
    "        \"<|END|>\": 100279,\n",
    "        \"<|DEL|>\": 100280,\n",
    "\n",
    "    }\n",
    ")\n",
    "print(tokenizer.n_vocab) #this is the number of tokens in our tokenizer\n",
    "print(tokenizer._special_tokens) #prints out our special tokens \n",
    "\n",
    "specials = {\"<|PAD|>\",\"<|START|>\",\"<|END|>\", \"<|DEL|>\"}\n",
    "\n",
    "print(dir(tokenizer))\n",
    "\n",
    "pad_token = 100277\n",
    "start_token = 100278\n",
    "end_token = 100279\n",
    "del_token = 100280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 16 #This is the value of T\n",
    "batch_size = 32 #This it the value of B\n",
    "n_embed = 512\n",
    "dropout = 0.1\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 50000\n",
    "\n",
    "beam_size = 7\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    num_examples = 750000\n",
    "\n",
    "    en_max = 0\n",
    "    en_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_en = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = line\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            en_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > en_max:\n",
    "                en_max = len(tok_sentence)\n",
    "                print(en_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[pad_token]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_en.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_en.append(block_size*[del_token])\n",
    "\n",
    "    en_length = torch.tensor(en_length).float()     \n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0\n",
    "    de_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_de = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            de_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > de_max:\n",
    "                de_max = len(tok_sentence)\n",
    "                print(de_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[pad_token]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_de.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_de.append(block_size*[del_token])\n",
    "\n",
    "    de_length = torch.tensor(de_length).float()               \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "\n",
    "    #combine the arrays together\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    #check for indices in both sentences that have rows containing the DEL token\n",
    "    idx = np.where(sentences == del_token)\n",
    "\n",
    "    #delete every row that contains the DEL token\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = torch.tensor(sentences[0], dtype=torch.long)\n",
    "    sentences_de = torch.tensor(sentences[1], dtype=torch.long)\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(f\"Average length of english tokenized sentence: {torch.mean(en_length):.4f}, and with std: {torch.std(en_length):.4f}\")\n",
    "    print(f\"Average length of german tokenized sentence: {torch.mean(de_length):.4f}, and with std: {torch.std(de_length):.4f}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    with open(os.getcwd()+'\\\\data\\\\deu.txt', 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "        print(len(f.readlines()))\n",
    "        lines = text.strip().split('\\n')\n",
    "        pairs = [line.split('\\t') for line in lines]\n",
    "        \n",
    "    eng = [pairs[i][0] for i in range(len(pairs))]\n",
    "    ger = [pairs[i][1] for i in range(len(pairs))]\n",
    "    en_max = 0\n",
    "    en_length = []\n",
    "    \n",
    "    sentences_en = []\n",
    "    for i in tqdm(range(152818)):\n",
    "        line = eng[i]\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = line\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        en_length.append(len(tok_sentence))\n",
    "        if len(tok_sentence) > en_max:\n",
    "            en_max = len(tok_sentence)\n",
    "            #print(en_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[pad_token]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            sentences_en.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_en.append(block_size*[del_token])\n",
    "\n",
    "    en_length = torch.tensor(en_length).float()     \n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0\n",
    "    de_length = []\n",
    "    sentences_de = []\n",
    "    for i in tqdm(range(152818)):\n",
    "        line = ger[i]\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        de_length.append(len(tok_sentence))\n",
    "        if len(tok_sentence) > de_max:\n",
    "            de_max = len(tok_sentence)\n",
    "            #print(de_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[pad_token]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            sentences_de.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_de.append(block_size*[del_token])\n",
    "\n",
    "    de_length = torch.tensor(de_length).float()               \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "\n",
    "    #combine the arrays together\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    #check for indices in both sentences that have rows containing the DEL token\n",
    "    idx = np.where(sentences == del_token)\n",
    "\n",
    "    #delete every row that contains the DEL token\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = torch.tensor(sentences[0], dtype=torch.long)\n",
    "    sentences_de = torch.tensor(sentences[1], dtype=torch.long)\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(f\"Average length of english tokenized sentence: {torch.mean(en_length):.4f}, and with std: {torch.std(en_length):.4f}\")\n",
    "    print(f\"Average length of german tokenized sentence: {torch.mean(de_length):.4f}, and with std: {torch.std(de_length):.4f}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create = False\n",
    "if create:\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND VAL DATASETS\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'rb') as f:\n",
    "    english_sentences = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'rb') as f:\n",
    "    german_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "print(train_data_en[0].shape)\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = pad_token)\n",
    "\n",
    "    x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, t = get_batch(\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def PositionalEncoding(seq_len, n_embd):\n",
    "        \n",
    "    pos_enc = torch.zeros(seq_len, n_embd)\n",
    "    position = torch.arange(0, seq_len, dtype = torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd, 2) * (-math.log(10000.0) / n_embd))\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pos_enc.to(device)\n",
    "\n",
    "def get_padding_mask_matrix(x, x_embed):\n",
    "    \"\"\"\n",
    "    x is the (B, T) tokenized matrix with padding included\n",
    "    x_embed is the embedded matrix that we will convert all rows to zero based on the corresponding row index = padding index\n",
    "    \"\"\"\n",
    "\n",
    "    #locate every index in each tokenized sentence which contains the pad index\n",
    "    pad_indices = torch.nonzero(x == pad_token).squeeze().to(device) #This will return a (N, 2) where the first column represents the sentence (batch_index) and the second column represents the corresponding index which is the pad index (This 2nd column represents which row we will set to all zeros)\n",
    "\n",
    "    #initialize a torch.ones of the shape of the embedding matrix\n",
    "    mask = torch.ones(x_embed.shape).to(device)\n",
    "\n",
    "    #For each row in the pad_indices matrix, we go to pad_indices[0] to grab the current batch example, and we go to the corresponding row of the batch example using the value of pad_indices[1]. We turn every column of that row into zeros\n",
    "    #ex: if the current row is [1, 4], then we go to the 2nd batch example, go to the 4th row, and wipe it clean with zeroes\n",
    "    mask[pad_indices[:,0], pad_indices[:,1], :] = 0\n",
    "\n",
    "    #element-wise product\n",
    "    x_padded = x_embed * mask\n",
    "\n",
    "    return x_padded.to(device)\n",
    "\n",
    "\n",
    "def apply_padding_mask(x, padding_token):\n",
    "    \"\"\"\n",
    "    Apply padding mask to input sequence, ignoring padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_sequence (torch.Tensor): Input sequence of shape (B, T).\n",
    "        padding_token: Padding token value used in the input sequence.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Padding mask of shape (B, T) with 0s for padding tokens and 1s for non-padding tokens.\n",
    "    \"\"\"\n",
    "    padding_mask = (x != padding_token).bool()\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamNode():\n",
    "    def __init__(self, tokens, score):\n",
    "        self.tokens = tokens #pytorch tensor\n",
    "        self.score = score #float\n",
    "        self.is_finished = False\n",
    "\n",
    "    def update(self, score):\n",
    "        self.score = score\n",
    "\n",
    "    def get_token(self):\n",
    "        return self.tokens\n",
    "\n",
    "    def print_(self):\n",
    "        print(f\"tokenized sentence: {self.tokens}, \", f\"log_prob score: {self.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, decoder = False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        if self.decoder:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data['x']\n",
    "        padding_mask = data['mask']\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        #assume input is of size (B, T, C)\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        attention_scores = attention_scores.masked_fill_(padding_mask.unsqueeze(1) == False, float('-inf'))\n",
    "\n",
    "        if self.decoder:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "            \n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class crossHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data['x']\n",
    "        enc_out = data['enc_out']\n",
    "        padding_mask = data['mask']\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        #assume x is of shape (B, T, C)\n",
    "        #assume enc_out is of shape (B, T, C)\n",
    "\n",
    "        K = self.Wk(enc_out) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(enc_out) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        attention_scores = attention_scores.masked_fill_(padding_mask.unsqueeze(1) == False, float('-inf'))\n",
    "\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "\n",
    "\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, decoder):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, decoder) for _ in range(n_heads)])\n",
    "        #output of heads is of size (B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(head_size * n_heads, n_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        out = torch.cat([h(data) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([crossHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = torch.cat([h(data) for h in self.heads], dim = -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed, padding_idx = pad_token, device = device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T)\n",
    "        return self.embedding(x.long()) * n_embed**(1/2)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4*n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = False)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data['x']\n",
    "        enc_mask = data['mask']\n",
    "        #assume input x is of size (B, T, C)\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(data) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x = self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return {'x': x, 'mask': enc_mask}\n",
    "\n",
    "class DecoderCrossBlock(nn.Module):\n",
    "    #one implementation of the multi head cross attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.heads = MultiHeadCrossAttention(head_size)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, data):\n",
    "        #assume parameters[0] is input of shape (B, T, C), It is the output of the decoder self attention layer\n",
    "        #assume parameters is a list of length 2: first element is the output of the previous hidden layer, and the 2nd element is the output of the encoder\n",
    "        #print(data)\n",
    "        x = data['x']\n",
    "        #print(x)\n",
    "        enc_out = data['enc_out']\n",
    "        dec_mask = data['mask']\n",
    "\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.heads(data) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return {'x': x, 'enc_out': enc_out, 'mask': dec_mask}\n",
    "\n",
    "class DecoderSelfBlock(nn.Module):\n",
    "    #one implementation of the multi head self attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_heads\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = True)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x = data['x']\n",
    "        dec_mask = data['mask']\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.sa(data)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return {'x': x, 'mask': dec_mask}\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix_x = Embedding()\n",
    "        self.tok_embedding_matrix_y = Embedding()\n",
    "\n",
    "        #positional embedding is a function that requires no backpropagation, so we don't need to initialize it in here\n",
    "        \n",
    "        self.EncoderBlocks = nn.Sequential(*[EncoderBlock() for _ in range(n_layers)])\n",
    "        self.DecoderSelfBlocks = nn.Sequential(*[DecoderSelfBlock() for _ in range(n_layers)])\n",
    "        self.DecoderCrossBlocks = nn.Sequential(*[DecoderCrossBlock() for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
    "        self.final_linear = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    #looks to be very brokenish? idk\n",
    "    def greedy_translate(self, x):\n",
    "        #assume input is a english sentence\n",
    "        tokenized_sentence = tokenizer.encode(x, allowed_special = specials)\n",
    "        #make sure the sentence is less than out block size\n",
    "        assert len(tokenized_sentence) <= block_size, print(\"this sentence is greater than our block_size\")\n",
    "\n",
    "        len_pad = block_size - len(tokenized_sentence)\n",
    "        tokenized_sentence = torch.tensor(tokenized_sentence + len_pad*[pad_token]).view(1, -1).to(device)\n",
    "        print(tokenized_sentence)\n",
    "\n",
    "        run = True\n",
    "\n",
    "        #initialize an output of size block_size with pad tokens\n",
    "        out = block_size*[pad_token]\n",
    "        out[0] = start_token\n",
    "        out = torch.tensor(out).view(1, -1).to(device)\n",
    "        i = 1\n",
    "        while run is True:\n",
    "            T = out.shape[1]\n",
    "\n",
    "            logits, loss = self(tokenized_sentence, out)\n",
    "            logits = logits[:, i, :] #becomes (B, C)\n",
    "            #apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            # print(\"top7 prob: \", torch.topk(probs, 20)[0])\n",
    "            # print(\"top7 idx: \", torch.topk(probs, 20)[1])\n",
    "            # print('ger:',tokenizer.decode((torch.topk(probs, 7)[1].int().tolist())[0]))\n",
    "            idx_next = torch.max(probs, dim = -1)[1] # (B, 1)\n",
    "            #out = torch.cat((out, idx_next), dim = 1) #(B, T+1)\n",
    "            out[0][i] = idx_next.item()\n",
    "            print('next token: ',idx_next)\n",
    "            #print(out[0].tolist())\n",
    "            #print(tokenizer.decode(out[0].tolist()))\n",
    "\n",
    "            i += 1\n",
    "            if idx_next == end_token or i == block_size:\n",
    "                run = False\n",
    "\n",
    "        #post processing\n",
    "        translated_sentence = out[0].tolist()\n",
    "        translated_sentence = tokenizer.decode(translated_sentence)\n",
    "        translated_sentence = translated_sentence.replace(\"<|PAD|>\",'')\n",
    "        return translated_sentence\n",
    "    \n",
    "    def beam_translate(self, x):\n",
    "        tokenized_sentence = tokenizer.encode(x, allowed_special = specials)\n",
    "        #make sure the sentence is less than out block size\n",
    "        assert len(tokenized_sentence) <= block_size, print(\"this sentence is greater than our block_size\")\n",
    "\n",
    "        len_pad = block_size - len(tokenized_sentence)\n",
    "        tokenized_sentence = torch.tensor(tokenized_sentence + len_pad*[pad_token]).view(1, -1).to(device)\n",
    "\n",
    "        out = block_size*[pad_token]\n",
    "        out[0] = start_token\n",
    "        out = torch.tensor(out).view(1, -1).to(device)\n",
    "\n",
    "        #initialize the beam nodes, each node will contain a tokenized pytorch tensor of tokens along with their corresponding log_score\n",
    "        beams = [BeamNode(torch.tensor(out, requires_grad=False), 0.0) for _ in range(beam_size)]\n",
    "        final_beams = beams #create a reference copy of the beam list, we will use this to re-extract the best beam from this var\n",
    "        logits, loss = self(tokenized_sentence, out) #generate the second token (first is SOS token)\n",
    "\n",
    "        #first pass\n",
    "        t = 1\n",
    "\n",
    "        logits = logits[:, t, :] #becomes (B, C)\n",
    "\n",
    "        #probs = F.softmax(logits, dim = -1)\n",
    "        log_probs = F.log_softmax(logits, dim = -1)\n",
    "        topk_probs, topk_indices = torch.topk(log_probs, beam_size) #grab the log_probs and indices of the best beam_size tokens\n",
    "\n",
    "        for i in range(len(beams)):\n",
    "            beams[i].tokens[0][t] = topk_indices[0][i].reshape(1) #change the 2nd token inplace (prob a better way to do this i guess)\n",
    "            beams[i].update(beams[i].score + topk_probs[0][i]) #update the score in the beam\n",
    "\n",
    "        #remaining passes\n",
    "        for t in range(2,block_size):\n",
    "            candidates = [] #at each time iteration, refresh the candidate list. This list will contain all the candidate token sentences and their corresponding log_scores as a 2 dimensional tuple\n",
    "            for i in range(len(beams)): #for each active beam (some beams can be deactivated if they reach the 'EOS' token)\n",
    "                logits, _ = self(tokenized_sentence,beams[i].tokens) #run the model on the current beam's tokens\n",
    "                logits = logits[:, t, :] #B, C \n",
    "                log_probs = F.log_softmax(logits, dim = -1) #calculate log probs\n",
    "                topk_probs, topk_indices = torch.topk(log_probs, beam_size) ##grab the log_probs and indices of the best beam_size tokens\n",
    "\n",
    "                for j in range(beam_size): #for each active candidate tokenized sentence (active means not closed, i.e 'EOS token is reached), grab the beam_size best possible next token and calculate their updated scores\n",
    "                    candidate_token = beams[i].get_token()[0][:t] #specify the candidate token as the original/CURRENT set of tokens UP TO time t, (basically we just take the original current beam tokens and remove all the <|PAD|> tokens)\n",
    "                    candidate_token = torch.cat((candidate_token, topk_indices[0][j].reshape(1))) #append the candidate token onto the curent set of tokens (up to time t)\n",
    "                    candidate_score = beams[i].score #grab the original set of tokens score\n",
    "                    candidates.append((candidate_token, candidate_score + topk_probs[0][j])) #append the new set of tokens and how high the score is\n",
    "\n",
    "            candidates_sorted = sorted(candidates, key=lambda x: x[1], reverse=True)[:len(beams)] #sort the candidate list by score. best is first\n",
    "\n",
    "            for i in range(len(beams)): #update all of the active beams to contain the best set of appended tokenized tensors\n",
    "                beams[i].update(candidates_sorted[i][1])\n",
    "                beams[i].tokens[0][:len(candidates_sorted[i][0])] = candidates_sorted[i][0]\n",
    "\n",
    "                if end_token in beams[i].tokens[0]: # if EOS token is appended, then we deactivate the beam\n",
    "                    beams[i].is_finished = True\n",
    "\n",
    "            beams = [beam for beam in beams if beam.is_finished != True] #for the next iteration of t, only work on the active beams\n",
    "            print(len(beams))\n",
    "\n",
    "        best_ = max(final_beams, key = lambda x: x.score).tokens #grab the best beam after beam search is over\n",
    "        translated_sentence = tokenizer.decode(best_.tolist()[0])\n",
    "        translated_sentence = translated_sentence.replace(\"<|PAD|>\",'')\n",
    "\n",
    "        return translated_sentence\n",
    "    \n",
    "    def forward(self, x, y, targets = None):\n",
    "        Bx, Tx = x.shape\n",
    "        Cx = n_embed\n",
    "        \n",
    "        tok_embed_x = self.tok_embedding_matrix_x(x)\n",
    "        pos_embed_x = PositionalEncoding(Tx, Cx)\n",
    "\n",
    "        By, Ty, = y.shape\n",
    "        Cy = n_embed\n",
    "\n",
    "        tok_embed_y = self.tok_embedding_matrix_y(y)\n",
    "        pos_embed_y = PositionalEncoding(Ty, Cy)\n",
    "\n",
    "        tok_pos_embed_x = tok_embed_x + pos_embed_x\n",
    "        tok_pos_embed_y = tok_embed_y + pos_embed_y\n",
    "        \n",
    "        # masked_tok_embed_x = get_padding_mask_matrix(x, tok_pos_embed_x)\n",
    "        # masked_tok_embed_y = get_padding_mask_matrix(y, tok_pos_embed_y)\n",
    "\n",
    "        enc_mask = apply_padding_mask(x, pad_token)\n",
    "        dec_mask = apply_padding_mask(y, pad_token)\n",
    "\n",
    "        x = tok_pos_embed_x\n",
    "        y = tok_pos_embed_y\n",
    "\n",
    "        x_in = {'x': x, 'mask': enc_mask}\n",
    "        y_in = {'x': y, 'mask': dec_mask}\n",
    "\n",
    "        #encoder\n",
    "        enc_out = self.EncoderBlocks(x_in)['x']\n",
    "\n",
    "        #decoder self\n",
    "        y = self.DecoderSelfBlocks(y_in)['x']\n",
    "\n",
    "        #decoder cross\n",
    "        #its ideal to send in one parameter only (i.e self, x) when passing parameters through stacked layers in an nn.Sequential, so we have to combine our previous hidden state output along with the enc_out into one object\n",
    "\n",
    "        in_ = {'x': y, 'enc_out': enc_out, 'mask': dec_mask}\n",
    "        y = self.DecoderCrossBlocks(in_)\n",
    "\n",
    "        #grab the transformed decoder input from the cross attention layer\n",
    "        y = y['x']\n",
    "        #remaining layers\n",
    "        y = self.final_layernorm(y)\n",
    "        logits = self.final_linear(y)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(By*Ty, -1)\n",
    "            targets = targets.view(targets.shape[0]*targets.shape[1])\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.848185 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = pad_token)\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb, tb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "\n",
    "            B, T, = yb.shape\n",
    "            C = n_embed\n",
    "\n",
    "            logits = logits.view(B*T, -1)\n",
    "            tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "            loss = loss_fn(logits, tb)\n",
    "            \n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.1646, val loss 0.1658, lr 0.0001\n",
      "step 500: train loss 0.1669, val loss 0.1698, lr 0.0001\n",
      "step 1000: train loss 0.1728, val loss 0.1686, lr 0.0001\n",
      "step 1500: train loss 0.1678, val loss 0.1698, lr 0.0001\n",
      "step 2000: train loss 0.1670, val loss 0.1707, lr 0.0001\n",
      "step 2500: train loss 0.1708, val loss 0.1701, lr 0.0001\n",
      "step 3000: train loss 0.1695, val loss 0.1678, lr 0.0001\n",
      "step 3500: train loss 0.1703, val loss 0.1684, lr 0.0001\n",
      "step 4000: train loss 0.1703, val loss 0.1693, lr 0.0001\n",
      "step 4500: train loss 0.1715, val loss 0.1691, lr 0.0001\n",
      "step 5000: train loss 0.1676, val loss 0.1684, lr 0.0001\n",
      "step 5500: train loss 0.1695, val loss 0.1710, lr 0.0001\n",
      "step 6000: train loss 0.1749, val loss 0.1699, lr 0.0001\n",
      "step 6500: train loss 0.1724, val loss 0.1701, lr 0.0001\n",
      "step 7000: train loss 0.1734, val loss 0.1740, lr 0.0001\n",
      "step 7500: train loss 0.1712, val loss 0.1684, lr 0.0001\n",
      "step 8000: train loss 0.1675, val loss 0.1661, lr 0.0001\n",
      "step 8500: train loss 0.1729, val loss 0.1728, lr 0.0001\n",
      "step 9000: train loss 0.1763, val loss 0.1688, lr 0.0001\n",
      "step 9500: train loss 0.1681, val loss 0.1720, lr 0.0001\n",
      "step 10000: train loss 0.1731, val loss 0.1692, lr 0.0001\n",
      "step 10500: train loss 0.1657, val loss 0.1714, lr 0.0001\n",
      "step 11000: train loss 0.1711, val loss 0.1778, lr 0.0001\n",
      "step 11500: train loss 0.1703, val loss 0.1743, lr 0.0001\n",
      "step 12000: train loss 0.1670, val loss 0.1683, lr 0.0001\n",
      "step 12500: train loss 0.1682, val loss 0.1723, lr 0.0001\n",
      "step 13000: train loss 0.1717, val loss 0.1713, lr 0.0001\n",
      "step 13500: train loss 0.1780, val loss 0.1674, lr 0.0001\n",
      "step 14000: train loss 0.1709, val loss 0.1681, lr 0.0001\n",
      "step 14500: train loss 0.1721, val loss 0.1726, lr 0.0001\n",
      "step 15000: train loss 0.1713, val loss 0.1700, lr 0.0001\n",
      "step 15500: train loss 0.1698, val loss 0.1670, lr 0.0001\n",
      "step 16000: train loss 0.1720, val loss 0.1682, lr 0.0001\n",
      "step 16500: train loss 0.1752, val loss 0.1714, lr 0.0001\n",
      "step 17000: train loss 0.1705, val loss 0.1679, lr 0.0001\n",
      "step 17500: train loss 0.1751, val loss 0.1722, lr 0.0001\n",
      "step 18000: train loss 0.1738, val loss 0.1719, lr 0.0001\n",
      "step 18500: train loss 0.1698, val loss 0.1725, lr 0.0001\n",
      "step 19000: train loss 0.1681, val loss 0.1678, lr 0.0001\n",
      "step 19500: train loss 0.1728, val loss 0.1714, lr 0.0001\n",
      "step 20000: train loss 0.1759, val loss 0.1745, lr 0.0001\n",
      "step 20500: train loss 0.1727, val loss 0.1707, lr 0.0001\n",
      "step 21000: train loss 0.1704, val loss 0.1695, lr 0.0001\n",
      "step 21500: train loss 0.1625, val loss 0.1650, lr 0.0001\n",
      "step 22000: train loss 0.1698, val loss 0.1716, lr 0.0001\n",
      "step 22500: train loss 0.1757, val loss 0.1670, lr 0.0001\n",
      "step 23000: train loss 0.1727, val loss 0.1700, lr 0.0001\n",
      "step 23500: train loss 0.1709, val loss 0.1730, lr 0.0001\n",
      "step 24000: train loss 0.1678, val loss 0.1701, lr 0.0001\n",
      "step 24500: train loss 0.1650, val loss 0.1681, lr 0.0001\n",
      "step 25000: train loss 0.1686, val loss 0.1729, lr 0.0001\n",
      "step 25500: train loss 0.1734, val loss 0.1704, lr 0.0001\n",
      "step 26000: train loss 0.1720, val loss 0.1681, lr 0.0001\n",
      "step 26500: train loss 0.1660, val loss 0.1711, lr 0.0001\n",
      "step 27000: train loss 0.1703, val loss 0.1696, lr 0.0001\n",
      "step 27500: train loss 0.1696, val loss 0.1676, lr 0.0001\n",
      "step 28000: train loss 0.1710, val loss 0.1675, lr 0.0001\n",
      "step 28500: train loss 0.1696, val loss 0.1736, lr 0.0001\n",
      "step 29000: train loss 0.1698, val loss 0.1743, lr 0.0001\n",
      "step 29500: train loss 0.1691, val loss 0.1673, lr 0.0001\n",
      "step 30000: train loss 0.1693, val loss 0.1675, lr 0.0001\n",
      "step 30500: train loss 0.1708, val loss 0.1727, lr 0.0001\n",
      "step 31000: train loss 0.1750, val loss 0.1710, lr 0.0001\n",
      "step 31500: train loss 0.1688, val loss 0.1682, lr 0.0001\n",
      "step 32000: train loss 0.1721, val loss 0.1705, lr 0.0001\n",
      "step 32500: train loss 0.1715, val loss 0.1750, lr 0.0001\n",
      "step 33000: train loss 0.1696, val loss 0.1726, lr 0.0001\n",
      "step 33500: train loss 0.1703, val loss 0.1681, lr 0.0001\n",
      "step 34000: train loss 0.1699, val loss 0.1747, lr 0.0001\n",
      "step 34500: train loss 0.1746, val loss 0.1681, lr 0.0001\n",
      "step 35000: train loss 0.1696, val loss 0.1689, lr 0.0001\n",
      "step 35500: train loss 0.1694, val loss 0.1674, lr 0.0001\n",
      "step 36000: train loss 0.1723, val loss 0.1661, lr 0.0001\n",
      "step 36500: train loss 0.1698, val loss 0.1697, lr 0.0001\n",
      "step 37000: train loss 0.1682, val loss 0.1719, lr 0.0001\n",
      "step 37500: train loss 0.1679, val loss 0.1720, lr 0.0001\n",
      "step 38000: train loss 0.1727, val loss 0.1662, lr 0.0001\n",
      "step 38500: train loss 0.1691, val loss 0.1739, lr 0.0001\n",
      "step 39000: train loss 0.1690, val loss 0.1712, lr 0.0001\n",
      "step 39500: train loss 0.1729, val loss 0.1673, lr 0.0001\n",
      "step 40000: train loss 0.1680, val loss 0.1720, lr 0.0001\n",
      "step 40500: train loss 0.1699, val loss 0.1667, lr 0.0001\n",
      "step 41000: train loss 0.1730, val loss 0.1724, lr 0.0001\n",
      "step 41500: train loss 0.1692, val loss 0.1701, lr 0.0001\n",
      "step 42000: train loss 0.1720, val loss 0.1737, lr 0.0001\n",
      "step 42500: train loss 0.1728, val loss 0.1662, lr 0.0001\n",
      "step 43000: train loss 0.1728, val loss 0.1684, lr 0.0001\n",
      "step 43500: train loss 0.1745, val loss 0.1686, lr 0.0001\n",
      "step 44000: train loss 0.1728, val loss 0.1714, lr 0.0001\n",
      "step 44500: train loss 0.1682, val loss 0.1723, lr 0.0001\n",
      "step 45000: train loss 0.1660, val loss 0.1671, lr 0.0001\n",
      "step 45500: train loss 0.1643, val loss 0.1679, lr 0.0001\n",
      "step 46000: train loss 0.1680, val loss 0.1709, lr 0.0001\n",
      "step 46500: train loss 0.1640, val loss 0.1700, lr 0.0001\n",
      "step 47000: train loss 0.1698, val loss 0.1677, lr 0.0001\n",
      "step 47500: train loss 0.1732, val loss 0.1706, lr 0.0001\n",
      "step 48000: train loss 0.1697, val loss 0.1716, lr 0.0001\n",
      "step 48500: train loss 0.1678, val loss 0.1684, lr 0.0001\n",
      "step 49000: train loss 0.1674, val loss 0.1719, lr 0.0001\n",
      "step 49500: train loss 0.1652, val loss 0.1698, lr 0.0001\n",
      "step 49999: train loss 0.1720, val loss 0.1707, lr 0.0001\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        lr_ = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr_}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb, tb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    B, T, = yb.shape\n",
    "    C = n_embed\n",
    "\n",
    "    logits = logits.view(B*T, -1)\n",
    "    tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "    loss = loss_fn(logits, tb)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved at: d:\\Documents\\Github\\robots\\machine_model\\model.pt\n"
     ]
    }
   ],
   "source": [
    "# filepath = os.getcwd()+\"\\\\machine_model\\\\model.pt\"\n",
    "# torch.save(model.state_dict(), filepath)\n",
    "# print(\"model saved at:\", filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"\\\\machine_model\\\\model.pt\"\n",
    "model = Transformer()\n",
    "model.load_state_dict(torch.load(filepath))\n",
    "model.eval()\n",
    "m = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    40,   4265,   1093,    264,  33566,     13, 100277, 100277, 100277,\n",
       "        100277, 100277, 100277, 100277, 100277, 100277, 100277])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100278,  26946,  28826,  15826,  99014,    342,  82284,     13,    220,\n",
       "        100279, 100277, 100277, 100277, 100277, 100277, 100277])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_sentences[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't see it.<|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|>\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(english_sentences[5000].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|START|> Ich sehe es nicht. <|END|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|>'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(german_sentences[5000].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  5618,   1541,    956,   1005,    856,    836, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0')\n",
      "next token:  tensor([14895], device='cuda:0')\n",
      "next token:  tensor([7141], device='cuda:0')\n",
      "next token:  tensor([8969], device='cuda:0')\n",
      "next token:  tensor([308], device='cuda:0')\n",
      "next token:  tensor([82312], device='cuda:0')\n",
      "next token:  tensor([713], device='cuda:0')\n",
      "next token:  tensor([7674], device='cuda:0')\n",
      "next token:  tensor([15010], device='cuda:0')\n",
      "next token:  tensor([0], device='cuda:0')\n",
      "next token:  tensor([6529], device='cuda:0')\n",
      "next token:  tensor([41762], device='cuda:0')\n",
      "next token:  tensor([13], device='cuda:0')\n",
      "next token:  tensor([13], device='cuda:0')\n",
      "next token:  tensor([0], device='cuda:0')\n",
      "next token:  tensor([100279], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Please don't use my name\"\n",
    "#sentence = 'hello'\n",
    "uh_oh = m.greedy_translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\AppData\\Local\\Temp\\ipykernel_2144\\3083744723.py:287: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  beams = [BeamNode(torch.tensor(out, requires_grad=False), 0.0) for _ in range(beam_size)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Please don't use my name\"\n",
    "#sentence = 'hello'\n",
    "uh_oh = m.beam_translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|START|>cksih nicht n meinen seinenihchen.<|END|>'"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uh_oh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZER TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100277, 358, 4265, 220]\n",
      "[769, 220]\n",
      "[40, 4265]\n",
      "[220]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing the data needs to be done extremely carefully\n",
    "#tiktoken is a byte processor which means they encode/tokenize by the bytes and not by the actual 'words' in the sentence\n",
    "#for example:\n",
    "print(tokenizer.encode(\"<|PAD|> I'd \", allowed_special = specials))\n",
    "print(tokenizer.encode(\"Id \", allowed_special = specials))\n",
    "print(tokenizer.encode(\"I'd\", allowed_special = specials))\n",
    "print(tokenizer.encode(\" \"))\n",
    "#all have different encodings\n",
    "\n",
    "#358 represents \" I\" and 4265 represents \"'d\"\n",
    "#220 represents ' ' and 769 represents \"Id\"\n",
    "#40 represents \"I\"\n",
    "\n",
    "#its important to encode things as close to the original sentence as possible to avoid mistokenization of the input sentence\n",
    "#for example, we want the model to learn \"I'd\" so we want 40 4265 and not [358 4265] as \"I'd\" or [100277 358 4268] as \"I'd\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 128\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (1,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    #x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, t = get_batch('train')\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100277,    220,    972,   4800,    433,   3782,    311,   1522,    430,\n",
       "          99572,    323,    813,   1274,    304,    279,   3814,   7860,    835,\n",
       "            567,     12,    567,    835,    567,    892,  20802,    872,    282,\n",
       "          36036,   3871,   1174,    323,   1101,    315,    872,  24875,   2652,\n",
       "            379,  12791,   1174,   1524,    682,    279,   3814,   7860,    835,\n",
       "            567,     12,    567,    835,    567,    892,   1051,    814,  23738,\n",
       "            872,    282,  36036,   3871,    662,    220, 100277, 100277, 100277,\n",
       "         100277]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = tok_embedding_matrix(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = get_padding_mask_matrix(xb, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(x_ == z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, n_embed, padding_idx = 100277, device = device)\n",
    "\n",
    "x = tok_embedding_matrix(xb) + PositionalEncoding(block_size, n_embed)\n",
    "y = tok_embedding_matrix(yb) + PositionalEncoding(block_size, n_embed)\n",
    "\n",
    "x = get_padding_mask_matrix(xb, x)\n",
    "y = get_padding_mask_matrix(yb, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Head(head_size = 64, decoder = False)\n",
    "e = MultiHeadSelfAttention(64, decoder = False)\n",
    "e = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_e = e(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Head(head_size = 64, decoder = True)\n",
    "d = MultiHeadSelfAttention(head_size = 64, decoder = True)\n",
    "d = DecoderSelfBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_d = d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = crossHead(head_size = 64)\n",
    "c = MultiHeadCrossAttention(head_size = 64)\n",
    "c = DecoderCrossBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_c = c([out_d, out_e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('-inf') in out_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('-inf') in out_c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = model(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat((out_e, out_d), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(xb, yb, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<|PAD|>\", allowed_special = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(english_sentences[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (2,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    #x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, t = get_batch('train')\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, 3)\n",
    "\n",
    "x = tok_embedding_matrix(xb)\n",
    "y = tok_embedding_matrix(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = torch.nonzero(xb == 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = torch.ones(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad[zero_indices[:,0],zero_indices[:,1], :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded = x * pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beam testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamNode():\n",
    "    def __init__(self, tokens, score):\n",
    "        self.tokens = tokens #pytorch tensor\n",
    "        self.score = score #float\n",
    "        self.is_finished = False\n",
    "\n",
    "    def update(self, score):\n",
    "        self.score = score\n",
    "\n",
    "    def get_token(self):\n",
    "        return self.tokens\n",
    "\n",
    "    def print_(self):\n",
    "        print(f\"tokenized sentence: {self.tokens}, \", f\"log_prob score: {self.score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 7\n",
    "x = \"Please don't use my name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\AppData\\Local\\Temp\\ipykernel_2144\\3443196241.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  beams = [BeamNode(torch.tensor(out, requires_grad=False), 0.0) for _ in range(beam_size)]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "tokenized_sentence = tokenizer.encode(x, allowed_special = specials)\n",
    "#make sure the sentence is less than out block size\n",
    "assert len(tokenized_sentence) <= block_size, print(\"this sentence is greater than our block_size\")\n",
    "\n",
    "len_pad = block_size - len(tokenized_sentence)\n",
    "tokenized_sentence = torch.tensor(tokenized_sentence + len_pad*[pad_token]).view(1, -1).to(device)\n",
    "\n",
    "out = block_size*[pad_token]\n",
    "out[0] = start_token\n",
    "out = torch.tensor(out).view(1, -1).to(device)\n",
    "beams = [BeamNode(torch.tensor(out, requires_grad=False), 0.0) for _ in range(beam_size)]\n",
    "beams_ = beams\n",
    "logits, loss = model(tokenized_sentence, out)\n",
    "\n",
    "#first pass\n",
    "t = 1\n",
    "\n",
    "logits = logits[:, t, :] #becomes (B, C)\n",
    "\n",
    "probs = F.softmax(logits, dim = -1)\n",
    "log_probs = F.log_softmax(logits, dim = -1)\n",
    "#print(torch.topk(probs, beam_size))\n",
    "topk_probs, topk_indices = torch.topk(log_probs, beam_size)\n",
    "\n",
    "for i in range(len(beams)):\n",
    "    beams[i].tokens[0][t] = topk_indices[0][i].reshape(1)\n",
    "    beams[i].update(beams[i].score + topk_probs[0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "<|START|>cksih nicht n meinen seinenihchen.<|END|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|>\n"
     ]
    }
   ],
   "source": [
    "#second pass\n",
    "#t = 4\n",
    "for t in range(2,block_size):\n",
    "    candidates = []\n",
    "    for i in range(len(beams)):\n",
    "        logits, _ = model(tokenized_sentence,beams[i].tokens)\n",
    "        logits = logits[:, t, :]\n",
    "        log_probs = F.log_softmax(logits, dim = -1)\n",
    "        topk_probs, topk_indices = torch.topk(log_probs, beam_size)\n",
    "\n",
    "        for j in range(beam_size):\n",
    "            candidate_token = beams[i].get_token()[0][:t]\n",
    "            candidate_token = torch.cat((candidate_token, topk_indices[0][j].reshape(1)))\n",
    "            candidate_score = beams[i].score\n",
    "            candidates.append((candidate_token, candidate_score + topk_probs[0][j]))\n",
    "\n",
    "    candidates_sorted = sorted(candidates, key=lambda x: x[1], reverse=True)[:len(beams)]\n",
    "\n",
    "    for i in range(len(beams)):\n",
    "        beams[i].update(candidates_sorted[i][1])\n",
    "        beams[i].tokens[0][:len(candidates_sorted[i][0])] = candidates_sorted[i][0]\n",
    "\n",
    "        if end_token in beams[i].tokens[0]:\n",
    "            beams[i].is_finished = True\n",
    "\n",
    "    beams = [beam for beam in beams if beam.is_finished != True]\n",
    "    print(len(beams))\n",
    "\n",
    "best_ = max(beams_, key = lambda x: x.score).tokens\n",
    "\n",
    "print(tokenizer.decode(best_.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100278,  14895,   7141,   8969,    308,  82312,  60328,   7141,   7674,\n",
       "             13, 100279, 100277, 100277, 100277, 100277, 100277]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'<|START|>cksih nicht n meinengecheniten! zu ihn..!<|END|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-13.6709, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beams_[2].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence: tensor([[100278,    451,    409,   6383,  68482, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -6.096290588378906\n",
      "tokenized sentence: tensor([[100278,  14895,   7141,   8969,  82312, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -6.659762382507324\n",
      "tokenized sentence: tensor([[100278,  14895,   7141,   8969,   8261, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -6.750249862670898\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(beams)):\n",
    "    beams[i].print_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|START|>de de Ver deung<|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|><|PAD|>'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([100278,    451,    409,   6383,    409,   2234, 100277, 100277, 100277,\n",
    "         100277, 100277, 100277, 100277, 100277, 100277, 100277])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2, -3, -4, -5]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = [-1,-2,-3,-4,-5]\n",
    "\n",
    "a[:len(b)] = b \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\AppData\\Local\\Temp\\ipykernel_2144\\513197359.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "#third pass\n",
    "t = 4\n",
    "\n",
    "candidates = []\n",
    "for i in range(len(beams)):\n",
    "    #calculate all the potential candidates for a certain beam\n",
    "    logits, _ = model(tokenized_sentence,beams[i].tokens)\n",
    "    logits = logits[:, t, :]\n",
    "    log_probs = F.log_softmax(logits, dim = -1)\n",
    "    topk_probs, topk_indices = torch.topk(log_probs, beam_size)\n",
    "\n",
    "    candidate_token = beams[i].tokens.clone()\n",
    "    candidate_score = beams[i].score\n",
    "    for j in range(beam_size):\n",
    "        candidate_token[0][t] = topk_indices[0][j].reshape(1)\n",
    "        candidates.append((candidate_token, candidate_score + topk_probs[0][j]))\n",
    "\n",
    "candidates_sorted = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "for i in range(beam_size):\n",
    "    beams[i].replace(candidates_sorted[i][0], candidates_sorted[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence: tensor([[100278,    380,    753,  18268,  32076, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -5.329845428466797\n",
      "tokenized sentence: tensor([[100278,   3675,  24459,  15148,   4838, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -5.539424896240234\n",
      "tokenized sentence: tensor([[100278,     72,  59419,  59419,  59419, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -5.891436576843262\n",
      "tokenized sentence: tensor([[100278,   3675,  24459,  15148,   4838, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -6.766961097717285\n",
      "tokenized sentence: tensor([[100278,   3675,  24459,  15148,   4838, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0'),  log_prob score: -6.7958526611328125\n"
     ]
    }
   ],
   "source": [
    "for i in range(beam_size):\n",
    "    beams[i].print_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.1571, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_ = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "candidates_[:5][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "tensor([[100278,   3675,      3, 100277,      3, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0')\n",
      "tensor([[100278,   3675,      3, 100277, 100277, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "b = a\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)\n",
    "beams[0].tokens[0][2] = 3\n",
    "\n",
    "candidate = beams[0].tokens.clone()\n",
    "candidate[0][4] = 3\n",
    "print(candidate)\n",
    "print(beams[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100278,   3675,      3, 100277, 100277, 100277, 100277, 100277, 100277,\n",
       "        100277, 100277, 100277, 100277, 100277, 100277, 100277],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beams[0].tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 15339,   1070, 100277, 100277, 100277, 100277, 100277, 100277, 100277,\n",
      "         100277, 100277, 100277, 100277, 100277, 100277, 100277]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#assume input is a english sentence\n",
    "tokenized_sentence = tokenizer.encode(x, allowed_special = specials)\n",
    "#make sure the sentence is less than out block size\n",
    "assert len(tokenized_sentence) <= block_size, print(\"this sentence is greater than our block_size\")\n",
    "\n",
    "len_pad = block_size - len(tokenized_sentence)\n",
    "tokenized_sentence = torch.tensor(tokenized_sentence + len_pad*[pad_token]).view(1, -1).to(device)\n",
    "print(tokenized_sentence)\n",
    "\n",
    "run = True\n",
    "\n",
    "#initialize an output of size block_size with pad tokens\n",
    "out = block_size*[pad_token]\n",
    "out[0] = start_token\n",
    "out = torch.tensor(out).view(1, -1).to(device)\n",
    "i = 1\n",
    "while run is True:\n",
    "    T = out.shape[1]\n",
    "\n",
    "    logits, loss = model(tokenized_sentence, out)\n",
    "    logits = logits[:, i, :] #becomes (B, C)\n",
    "    #apply softmax to get the probabilities\n",
    "    probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "    #sample from the distribution\n",
    "    # print(\"top7 prob: \", torch.topk(probs, 20)[0])\n",
    "    # print(\"top7 idx: \", torch.topk(probs, 20)[1])\n",
    "    # print('ger:',tokenizer.decode((torch.topk(probs, 7)[1].int().tolist())[0]))\n",
    "    idx_next = torch.max(probs, dim = -1)[1] # (B, 1)\n",
    "    #out = torch.cat((out, idx_next), dim = 1) #(B, T+1)\n",
    "    out[0][i] = idx_next.item()\n",
    "    print('next token: ',idx_next)\n",
    "    #print(out[0].tolist())\n",
    "    #print(tokenizer.decode(out[0].tolist()))\n",
    "\n",
    "    i += 1\n",
    "    if idx_next == end_token or i == block_size:\n",
    "        run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100281])\n",
      "top7 prob:  tensor([[5.6157e-05, 5.5625e-05, 5.3637e-05, 5.3094e-05, 5.3035e-05, 5.0744e-05,\n",
      "         4.9988e-05, 4.9316e-05, 4.8993e-05, 4.7038e-05, 4.6860e-05, 4.5850e-05,\n",
      "         4.5161e-05, 4.4819e-05, 4.4678e-05, 4.3743e-05, 4.3328e-05, 4.3037e-05,\n",
      "         4.3021e-05, 4.2896e-05]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "top7 idx:  tensor([[75122, 49842, 65741,  2642, 41264, 25839, 71966, 31732, 18017, 21124,\n",
      "         63982, 49981, 87437, 18853, 24884, 29664, 69622, 71430, 29328,  2452]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(xb, yb)\n",
    "\n",
    "logits = logits[:, 0, :] #becomes (B, C)\n",
    "\n",
    "print(logits.shape)\n",
    "#apply softmax to get the probabilities\n",
    "probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "#sample from the distribution\n",
    "print(\"top7 prob: \", torch.topk(probs, 20)[0])\n",
    "print(\"top7 idx: \", torch.topk(probs, 20)[1])\n",
    "# print('ger:',tokenizer.decode((torch.topk(probs, 7)[1].int().tolist())[0]))\n",
    "idx_next = torch.max(probs, dim = -1)[1] # (B, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
