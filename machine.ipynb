{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100281\n",
      "{'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276, '<|PAD|>': 100277, '<|START|>': 100278, '<|END|>': 100279, '<|DEL|>': 100280}\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_core_bpe', '_encode_bytes', '_encode_only_native_bpe', '_encode_single_piece', '_mergeable_ranks', '_pat_str', '_special_tokens', 'decode', 'decode_batch', 'decode_bytes', 'decode_bytes_batch', 'decode_single_token_bytes', 'decode_tokens_bytes', 'encode', 'encode_batch', 'encode_ordinary', 'encode_ordinary_batch', 'encode_single_token', 'encode_with_unstable', 'eot_token', 'max_token_value', 'n_vocab', 'name', 'special_tokens_set', 'token_byte_values']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#dataset https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "#tiktoken api https://github.com/openai/tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|PAD|>\": 100277,\n",
    "        \"<|START|>\": 100278,\n",
    "        \"<|END|>\": 100279,\n",
    "        \"<|DEL|>\": 100280,\n",
    "\n",
    "    }\n",
    ")\n",
    "print(tokenizer.n_vocab) #this is the number of tokens in our tokenizer\n",
    "print(tokenizer._special_tokens) #prints out our special tokens \n",
    "\n",
    "specials = {\"<|PAD|>\",\"<|START|>\",\"<|END|>\", \"<|DEL|>\"}\n",
    "\n",
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 128\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 50000\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    num_examples = 750000\n",
    "\n",
    "    en_max = 0\n",
    "    en_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_en = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            en_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > en_max:\n",
    "                en_max = len(tok_sentence)\n",
    "                print(en_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[100277]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_en.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_en.append(block_size*[100280])\n",
    "\n",
    "    en_length = torch.tensor(en_length).float()     \n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0\n",
    "    de_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_de = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            de_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > de_max:\n",
    "                de_max = len(tok_sentence)\n",
    "                print(de_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[100277]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_de.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_de.append(block_size*[100280])\n",
    "\n",
    "    de_length = torch.tensor(de_length).float()               \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "\n",
    "    #combine the arrays together\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    #check for indices in both sentences that have rows containing the DEL token\n",
    "    idx = np.where(sentences == 100280)\n",
    "\n",
    "    #delete every row that contains the DEL token\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = torch.tensor(sentences[0], dtype=torch.long)\n",
    "    sentences_de = torch.tensor(sentences[1], dtype=torch.long)\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(f\"Average length of english tokenized sentence: {torch.mean(en_length):.4f}, and with std: {torch.std(en_length):.4f}\")\n",
    "    print(f\"Average length of german tokenized sentence: {torch.mean(de_length):.4f}, and with std: {torch.std(de_length):.4f}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create = False\n",
    "if create:\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND VAL DATASETS\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'rb') as f:\n",
    "    english_sentences = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'rb') as f:\n",
    "    german_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def PositionalEncoding(seq_len, n_embd):\n",
    "        \n",
    "    pos_enc = torch.zeros(seq_len, n_embd)\n",
    "    position = torch.arange(0, seq_len, dtype = torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd, 2) * (-math.log(10000.0) / n_embd))\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pos_enc.to(device)\n",
    "\n",
    "def get_padding_mask_matrix(x, x_embed):\n",
    "    \"\"\"\n",
    "    x is the (B, T) tokenized matrix with padding included\n",
    "    x_embed is the embedded matrix that we will convert all rows to zero based on the corresponding row index = padding index\n",
    "    \"\"\"\n",
    "\n",
    "    #locate every index in each tokenized sentence which contains the pad index\n",
    "    pad_indices = torch.nonzero(x == 100277).squeeze().to(device) #This will return a (N, 2) where the first column represents the sentence (batch_index) and the second column represents the corresponding index which is the pad index (This 2nd column represents which row we will set to all zeros)\n",
    "\n",
    "    #initialize a torch.ones of the shape of the embedding matrix\n",
    "    mask = torch.ones(x_embed.shape).to(device)\n",
    "\n",
    "    #For each row in the pad_indices matrix, we go to pad_indices[0] to grab the current batch example, and we go to the corresponding row of the batch example using the value of pad_indices[1]. We turn every column of that row into zeros\n",
    "    #ex: if the current row is [1, 4], then we go to the 2nd batch example, go to the 4th row, and wipe it clean with zeroes\n",
    "    mask[pad_indices[:,0], pad_indices[:,1], :] = 0\n",
    "\n",
    "    #element-wise product\n",
    "    x_padded = x_embed * mask\n",
    "\n",
    "    return x_padded.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, decoder = False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        if self.decoder:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        #assume input is of size (B, T, C)\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        if self.decoder:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class crossHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out):\n",
    "        B, T, C = x.shape\n",
    "        #assume x is of shape (B, T, C)\n",
    "        #assume enc_out is of shape (B, T, C)\n",
    "\n",
    "        K = self.Wk(enc_out) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(enc_out) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "        # print(\"attn\",attention_scores.shape)\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, decoder):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, decoder) for _ in range(n_heads)])\n",
    "        #output of heads is of size (B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(head_size * n_heads, n_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([crossHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        x = torch.cat([h(x, enc_out) for h in self.heads], dim = -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed, padding_idx = 100277, device = device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T)\n",
    "        return self.embedding(x.long()) * n_embed**(1/2)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4*n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = False)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x = self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderCrossBlock(nn.Module):\n",
    "    #one implementation of the multi head cross attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.heads = MultiHeadCrossAttention(head_size)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, parameters):\n",
    "        #assume parameters[0] is input of shape (B, T, C), It is the output of the decoder self attention layer\n",
    "        #assume parameters is a list of length 2: first element is the output of the previous hidden layer, and the 2nd element is the output of the encoder\n",
    "        \n",
    "        x = parameters[0]\n",
    "        enc_out = parameters[1]\n",
    "        \n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.heads(x, enc_out) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return [x, enc_out]\n",
    "\n",
    "class DecoderSelfBlock(nn.Module):\n",
    "    #one implementation of the multi head self attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_heads\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = True)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix_x = Embedding()\n",
    "        self.tok_embedding_matrix_y = Embedding()\n",
    "\n",
    "        #positional embedding is a function that requires no backpropagation, so we don't need to initialize it in here\n",
    "        \n",
    "        self.EncoderBlocks = nn.Sequential(*[EncoderBlock() for _ in range(n_layers)])\n",
    "        self.DecoderSelfBlocks = nn.Sequential(*[DecoderSelfBlock() for _ in range(n_layers)])\n",
    "        self.DecoderCrossBlocks = nn.Sequential(*[DecoderCrossBlock() for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
    "        self.final_linear = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    #looks to be very broken\n",
    "    def translate(self, x):\n",
    "        #assume input is a english sentence\n",
    "        tokenized_sentence = tokenizer.encode(x, allowed_special = specials)\n",
    "        #make sure the sentence is less than out block size\n",
    "        assert len(tokenized_sentence) <= block_size, print(\"this sentence is greater than our block_size\")\n",
    "\n",
    "        len_pad = block_size - len(tokenized_sentence)\n",
    "        tokenized_sentence = torch.tensor(tokenized_sentence + len_pad*[0]).view(1, -1).to(device)\n",
    "        run = True\n",
    "        input = [100278]\n",
    "        while run is True:\n",
    "            input = torch.tensor(input).view(1, -1).to(device)\n",
    "            T = input.shape[1]\n",
    "            logits, loss = self(tokenized_sentence[:,:T], input)\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            input = torch.cat((input, idx_next), dim = 1) #(B, T+1)\n",
    "\n",
    "            if idx_next == 100279:\n",
    "                run = False\n",
    "\n",
    "        return input[0].tolist()\n",
    "            \n",
    "    def forward(self, x, y, targets = None):\n",
    "        Bx, Tx = x.shape\n",
    "        Cx = n_embed\n",
    "        \n",
    "        tok_embed_x = self.tok_embedding_matrix_x(x)\n",
    "        pos_embed_x = PositionalEncoding(Tx, Cx)\n",
    "\n",
    "        By, Ty, = y.shape\n",
    "        Cy = n_embed\n",
    "\n",
    "        tok_embed_y = self.tok_embedding_matrix_y(y)\n",
    "        pos_embed_y = PositionalEncoding(Ty, Cy)\n",
    "\n",
    "        tok_pos_embed_x = tok_embed_x + pos_embed_x\n",
    "        tok_pos_embed_y = tok_embed_y + pos_embed_y\n",
    "        \n",
    "        masked_tok_embed_x = get_padding_mask_matrix(x, tok_pos_embed_x)\n",
    "        masked_tok_embed_y = get_padding_mask_matrix(y, tok_pos_embed_y)\n",
    "\n",
    "        x = masked_tok_embed_x\n",
    "        y = masked_tok_embed_y\n",
    "\n",
    "        #encoder\n",
    "        enc_out = self.EncoderBlocks(x)\n",
    "\n",
    "        #decoder self\n",
    "        y = self.DecoderSelfBlocks(y)\n",
    "        #decoder cross\n",
    "        #its ideal to send in one parameter only (i.e self, x) when passing parameters through stacked layers in an nn.Sequential, so we have to combine our previous hidden state output along with the enc_out into one object\n",
    "        y = self.DecoderCrossBlocks([y, enc_out])\n",
    "\n",
    "        #grab the transformed decoder input from the cross attention layer\n",
    "        y = y[0]\n",
    "        #remaining layers\n",
    "        y = self.final_layernorm(y)\n",
    "        logits = self.final_linear(y)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(By*Ty, -1)\n",
    "            targets = targets.view(targets.shape[0]*targets.shape[1])\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.170425 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.0001, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617]}]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = 100277)\n",
    "\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb, tb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "\n",
    "            B, T, = yb.shape\n",
    "            C = n_embed\n",
    "\n",
    "            logits = logits.view(B*T, -1)\n",
    "            tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "            loss = F.cross_entropy(logits, tb)\n",
    "            \n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8424, val loss 1.8164, lr 0.0001\n",
      "step 500: train loss 1.7881, val loss 1.8114, lr 0.0001\n",
      "step 1000: train loss 1.8002, val loss 1.8107, lr 0.0001\n",
      "step 1500: train loss 1.8035, val loss 1.8099, lr 0.0001\n",
      "step 2000: train loss 1.8062, val loss 1.8037, lr 0.0001\n",
      "step 2500: train loss 1.8156, val loss 1.7988, lr 0.0001\n",
      "step 3000: train loss 1.8302, val loss 1.8207, lr 0.0001\n",
      "step 3500: train loss 1.8014, val loss 1.8098, lr 0.0001\n",
      "step 4000: train loss 1.8094, val loss 1.7967, lr 0.0001\n",
      "step 4500: train loss 1.8355, val loss 1.8183, lr 0.0001\n",
      "step 5000: train loss 1.8259, val loss 1.8007, lr 0.0001\n",
      "step 5500: train loss 1.8357, val loss 1.8045, lr 0.0001\n",
      "step 6000: train loss 1.8121, val loss 1.8170, lr 0.0001\n",
      "step 6500: train loss 1.8150, val loss 1.8269, lr 0.0001\n",
      "step 7000: train loss 1.8102, val loss 1.8441, lr 0.0001\n",
      "step 7500: train loss 1.8240, val loss 1.8152, lr 0.0001\n",
      "step 8000: train loss 1.8072, val loss 1.7784, lr 0.0001\n",
      "step 8500: train loss 1.8140, val loss 1.8267, lr 0.0001\n",
      "step 9000: train loss 1.8318, val loss 1.7876, lr 0.0001\n",
      "step 9500: train loss 1.8319, val loss 1.8246, lr 0.0001\n",
      "step 10000: train loss 1.8295, val loss 1.8201, lr 0.0001\n",
      "step 10500: train loss 1.8258, val loss 1.8153, lr 0.0001\n",
      "step 11000: train loss 1.7788, val loss 1.8157, lr 0.0001\n",
      "step 11500: train loss 1.8363, val loss 1.8083, lr 0.0001\n",
      "step 12000: train loss 1.8144, val loss 1.8469, lr 0.0001\n",
      "step 12500: train loss 1.8057, val loss 1.8177, lr 0.0001\n",
      "step 13000: train loss 1.8131, val loss 1.8008, lr 0.0001\n",
      "step 13500: train loss 1.8262, val loss 1.8082, lr 0.0001\n",
      "step 14000: train loss 1.8360, val loss 1.8437, lr 0.0001\n",
      "step 14500: train loss 1.8404, val loss 1.7979, lr 0.0001\n",
      "step 15000: train loss 1.8182, val loss 1.7905, lr 0.0001\n",
      "step 15500: train loss 1.8152, val loss 1.7805, lr 0.0001\n",
      "step 16000: train loss 1.8206, val loss 1.8234, lr 0.0001\n",
      "step 16500: train loss 1.8198, val loss 1.8276, lr 0.0001\n",
      "step 17000: train loss 1.8271, val loss 1.7824, lr 0.0001\n",
      "step 17500: train loss 1.8087, val loss 1.8253, lr 0.0001\n",
      "step 18000: train loss 1.8073, val loss 1.8109, lr 0.0001\n",
      "step 18500: train loss 1.8058, val loss 1.8136, lr 0.0001\n",
      "step 19000: train loss 1.8386, val loss 1.8018, lr 0.0001\n",
      "step 19500: train loss 1.8180, val loss 1.8299, lr 0.0001\n",
      "step 20000: train loss 1.8129, val loss 1.8389, lr 0.0001\n",
      "step 20500: train loss 1.8502, val loss 1.8313, lr 0.0001\n",
      "step 21000: train loss 1.7959, val loss 1.8063, lr 0.0001\n",
      "step 21500: train loss 1.8271, val loss 1.7779, lr 0.0001\n",
      "step 22000: train loss 1.8034, val loss 1.8279, lr 0.0001\n",
      "step 22500: train loss 1.7848, val loss 1.8109, lr 0.0001\n",
      "step 23000: train loss 1.8430, val loss 1.8193, lr 0.0001\n",
      "step 23500: train loss 1.8088, val loss 1.8114, lr 0.0001\n",
      "step 24000: train loss 1.8053, val loss 1.8227, lr 0.0001\n",
      "step 24500: train loss 1.8371, val loss 1.7907, lr 0.0001\n",
      "step 25000: train loss 1.8037, val loss 1.8290, lr 0.0001\n",
      "step 25500: train loss 1.8226, val loss 1.8268, lr 0.0001\n",
      "step 26000: train loss 1.8312, val loss 1.8019, lr 0.0001\n",
      "step 26500: train loss 1.8168, val loss 1.8324, lr 0.0001\n",
      "step 27000: train loss 1.8177, val loss 1.8063, lr 0.0001\n",
      "step 27500: train loss 1.7999, val loss 1.8042, lr 0.0001\n",
      "step 28000: train loss 1.8530, val loss 1.8170, lr 0.0001\n",
      "step 28500: train loss 1.8451, val loss 1.8262, lr 0.0001\n",
      "step 29000: train loss 1.8036, val loss 1.8315, lr 0.0001\n",
      "step 29500: train loss 1.8231, val loss 1.8218, lr 0.0001\n",
      "step 30000: train loss 1.8240, val loss 1.8402, lr 0.0001\n",
      "step 30500: train loss 1.8157, val loss 1.8094, lr 0.0001\n",
      "step 31000: train loss 1.8162, val loss 1.7969, lr 0.0001\n",
      "step 31500: train loss 1.8270, val loss 1.7887, lr 0.0001\n",
      "step 32000: train loss 1.8442, val loss 1.8009, lr 0.0001\n",
      "step 32500: train loss 1.8524, val loss 1.8183, lr 0.0001\n",
      "step 33000: train loss 1.8428, val loss 1.7975, lr 0.0001\n",
      "step 33500: train loss 1.8576, val loss 1.8078, lr 0.0001\n",
      "step 34000: train loss 1.8324, val loss 1.8241, lr 0.0001\n",
      "step 34500: train loss 1.8434, val loss 1.8136, lr 0.0001\n",
      "step 35000: train loss 1.7958, val loss 1.8283, lr 0.0001\n",
      "step 35500: train loss 1.8294, val loss 1.8081, lr 0.0001\n",
      "step 36000: train loss 1.8235, val loss 1.8238, lr 0.0001\n",
      "step 36500: train loss 1.8014, val loss 1.7929, lr 0.0001\n",
      "step 37000: train loss 1.8018, val loss 1.8229, lr 0.0001\n",
      "step 37500: train loss 1.8114, val loss 1.8386, lr 0.0001\n",
      "step 38000: train loss 1.8244, val loss 1.8275, lr 0.0001\n",
      "step 38500: train loss 1.8266, val loss 1.8155, lr 0.0001\n",
      "step 39000: train loss 1.8330, val loss 1.8285, lr 0.0001\n",
      "step 39500: train loss 1.8140, val loss 1.8243, lr 0.0001\n",
      "step 40000: train loss 1.8099, val loss 1.8341, lr 0.0001\n",
      "step 40500: train loss 1.8102, val loss 1.8099, lr 0.0001\n",
      "step 41000: train loss 1.8251, val loss 1.8127, lr 0.0001\n",
      "step 41500: train loss 1.7915, val loss 1.8224, lr 0.0001\n",
      "step 42000: train loss 1.8164, val loss 1.8010, lr 0.0001\n",
      "step 42500: train loss 1.8330, val loss 1.8335, lr 0.0001\n",
      "step 43000: train loss 1.8227, val loss 1.8072, lr 0.0001\n",
      "step 43500: train loss 1.8188, val loss 1.8280, lr 0.0001\n",
      "step 44000: train loss 1.8226, val loss 1.8182, lr 0.0001\n",
      "step 44500: train loss 1.8210, val loss 1.8056, lr 0.0001\n",
      "step 45000: train loss 1.8119, val loss 1.8524, lr 0.0001\n",
      "step 45500: train loss 1.8441, val loss 1.8152, lr 0.0001\n",
      "step 46000: train loss 1.8073, val loss 1.8044, lr 0.0001\n",
      "step 46500: train loss 1.8272, val loss 1.8261, lr 0.0001\n",
      "step 47000: train loss 1.8145, val loss 1.8067, lr 0.0001\n",
      "step 47500: train loss 1.8254, val loss 1.8087, lr 0.0001\n",
      "step 48000: train loss 1.8182, val loss 1.8327, lr 0.0001\n",
      "step 48500: train loss 1.8337, val loss 1.8538, lr 0.0001\n",
      "step 49000: train loss 1.8355, val loss 1.8112, lr 0.0001\n",
      "step 49500: train loss 1.8257, val loss 1.8314, lr 0.0001\n",
      "step 49999: train loss 1.8374, val loss 1.8149, lr 0.0001\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        lr_ = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr_}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb, tb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    B, T, = yb.shape\n",
    "    C = n_embed\n",
    "\n",
    "    logits = logits.view(B*T, -1)\n",
    "    tb = tb.view(tb.shape[0]*tb.shape[1])\n",
    "    loss = F.cross_entropy(logits, tb)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved at: d:\\Documents\\Github\\robots\\machine_model\\model.pt\n"
     ]
    }
   ],
   "source": [
    "# filepath = os.getcwd()+\"\\\\machine_model\\\\model.pt\"\n",
    "# torch.save(model.state_dict(), filepath)\n",
    "# print(\"model saved at:\", filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"\\\\machine_model\\\\model.pt\"\n",
    "model = Transformer()\n",
    "model.load_state_dict(torch.load(filepath))\n",
    "model.eval()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100277,  11245,  24532,    374,    264,   5644,    369,   1005,  25982,\n",
       "           902,    374,  17551,    439,    264,   1488,   1169,    555,   2231,\n",
       "          1919,  22145,    477,  14654,    304,    279,  51370,  13116,    320,\n",
       "         24359,    883,    315,    279,   9699,   6892,    354,  51370,    662,\n",
       "           220, 100277, 100277, 100277, 100277, 100277, 100277, 100277, 100277,\n",
       "        100277, 100277, 100277, 100277, 100277, 100277, 100277, 100277, 100277,\n",
       "        100277, 100277, 100277, 100277, 100277, 100277, 100277, 100277, 100277,\n",
       "        100277])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'T' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tok \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(sentence, allowed_special\u001b[39m=\u001b[39mspecials)\n\u001b[0;32m      4\u001b[0m \u001b[39mlen\u001b[39m(tok)\n\u001b[1;32m----> 5\u001b[0m uh_oh \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39;49mtranslate(sentence)\n",
      "Cell \u001b[1;32mIn[112], line 224\u001b[0m, in \u001b[0;36mTransformer.translate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m [\u001b[39m100278\u001b[39m]\n\u001b[0;32m    222\u001b[0m \u001b[39mwhile\u001b[39;00m run \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mprint\u001b[39m(tokenized_sentence[:,:T])\n\u001b[0;32m    226\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    227\u001b[0m     T \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'T' referenced before assignment"
     ]
    }
   ],
   "source": [
    "sentence = 'iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .'\n",
    "\n",
    "tok = tokenizer.encode(sentence, allowed_special=specials)\n",
    "len(tok)\n",
    "uh_oh = m.translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|START|> Clean and comfortable accommodation with excellent access to the shops and knowledgeableANN city . <|END|>'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(uh_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 128\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    #x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, t = get_batch('train')\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "x = tok_embedding_matrix(xb)\n",
    "y = tok_embedding_matrix(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Head(head_size = 64, decoder = False)\n",
    "e = MultiHeadSelfAttention(64, decoder = False)\n",
    "e = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_e = e(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Head(head_size = 64, decoder = True)\n",
    "#d = MultiHeadSelfAttention(head_size = 64, decoder = True)\n",
    "#d = DecoderSelfBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = crossHead(head_size = 64)\n",
    "c = MultiHeadCrossAttention(head_size = 64)\n",
    "c = DecoderCrossBlock()\n",
    "d = DecoderCrossBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat((out_e, out_d), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(xb, yb, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<|PAD|>\", allowed_special = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(english_sentences[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 10\n",
    "# num_examples = 5\n",
    "\n",
    "# en_max = 0 \n",
    "# with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "#     idx_en = []\n",
    "#     sentences_en = []\n",
    "#     for i in tqdm(range(num_examples)):\n",
    "#         line = f.readline()\n",
    "#         line = line.replace(\"\\n\", \"\")\n",
    "#         len_pad = 0\n",
    "#         sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "#         print(sentence)\n",
    "#         print(tokenizer.encode(sentence, allowed_special = specials))\n",
    "#         tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "#         print(len(tok_sentence))\n",
    "#         if len(tok_sentence) > en_max:\n",
    "#             en_max = len(tok_sentence)\n",
    "#             print(en_max)\n",
    "\n",
    "#         if len(tok_sentence) <= block_size:\n",
    "#             len_pad = block_size - len(tok_sentence)\n",
    "#             tok_sentence = tok_sentence + len_pad*[100277]\n",
    "#             assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "#             #idx_en.append(i)\n",
    "#             sentences_en.append(tok_sentence)\n",
    "#         else:\n",
    "#             sentences_en.append(block_size*[100280])\n",
    "\n",
    "# print(en_max)    \n",
    "# print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "\n",
    "# de_max = 0 \n",
    "# with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "#     idx_de = []\n",
    "#     sentences_de = []\n",
    "#     for i in tqdm(range(num_examples)):\n",
    "#         line = f.readline()\n",
    "#         line = line.replace(\"\\n\", \"\")\n",
    "#         len_pad = 0\n",
    "#         sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "#         tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "#         if len(tok_sentence) > de_max:\n",
    "#             de_max = len(tok_sentence)\n",
    "#             print(de_max)\n",
    "\n",
    "#         if len(tok_sentence) <= block_size:\n",
    "#             len_pad = block_size - len(tok_sentence)\n",
    "#             tok_sentence = tok_sentence + len_pad*[100277]\n",
    "#             assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "#             #idx_en.append(i)\n",
    "#             sentences_de.append(tok_sentence)\n",
    "#         else:\n",
    "#             sentences_de.append(block_size*[100280])\n",
    "            \n",
    "# print(de_max)  \n",
    "# print(f\"Length of sentences: {len(sentences_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = np.array([sentences_en, sentences_de])\n",
    "\n",
    "# idx = np.where(sentences == 100280)\n",
    "# print(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = np.delete(sentences, idx[1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_zero_indices(lst):\n",
    "    tensor = torch.tensor(lst)\n",
    "    zero_indices = torch.nonzero(tensor == 0).squeeze()\n",
    "    split_indices = torch.nonzero(torch.diff(zero_indices) != 1).squeeze() + 1\n",
    "    split_lists = torch.split(zero_indices, split_indices)\n",
    "\n",
    "    return [list(indices.numpy()) for indices in split_lists]\n",
    "\n",
    "# Example usage\n",
    "list1 = [[1, 2, 4, 0, 0, 0],[0, 4, 0, 2, 5, 8]]\n",
    "list2 = [0, 4, 0, 2, 5, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor(list1)\n",
    "zero_indices = torch.nonzero(p == 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(2, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[zero_indices[:,0],zero_indices[:,1], :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (2,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    #x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb, t = get_batch('train')\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, 3)\n",
    "\n",
    "x = tok_embedding_matrix(xb)\n",
    "y = tok_embedding_matrix(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = torch.nonzero(xb == 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = torch.ones(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad[zero_indices[:,0],zero_indices[:,1], :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded = x * pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
