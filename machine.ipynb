{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100282\n",
      "{'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276, '<|PAD|>': 0, '<|START|>': 100278, '<|END|>': 100279, '<|DEL|>': 100280, '!': 100281}\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_core_bpe', '_encode_bytes', '_encode_only_native_bpe', '_encode_single_piece', '_mergeable_ranks', '_pat_str', '_special_tokens', 'decode', 'decode_batch', 'decode_bytes', 'decode_bytes_batch', 'decode_single_token_bytes', 'decode_tokens_bytes', 'encode', 'encode_batch', 'encode_ordinary', 'encode_ordinary_batch', 'encode_single_token', 'encode_with_unstable', 'eot_token', 'max_token_value', 'n_vocab', 'name', 'special_tokens_set', 'token_byte_values']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#dataset https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "#tiktoken api https://github.com/openai/tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|PAD|>\": 0,\n",
    "        \"<|START|>\": 100278,\n",
    "        \"<|END|>\": 100279,\n",
    "        \"<|DEL|>\": 100280,\n",
    "        \"!\": 100281\n",
    "    }\n",
    ")\n",
    "print(tokenizer.n_vocab) #this is the number of tokens in our tokenizer\n",
    "print(tokenizer._special_tokens) #prints out our special tokens \n",
    "\n",
    "specials = {\"<|PAD|>\",\"<|START|>\",\"<|END|>\", \"<|DEL|>\", \"!\"}\n",
    "\n",
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 128\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 15000\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    num_examples = 100000\n",
    "\n",
    "    en_max = 0\n",
    "    en_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_en = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            en_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > en_max:\n",
    "                en_max = len(tok_sentence)\n",
    "                print(en_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_en.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_en.append(block_size*[100280])\n",
    "\n",
    "    en_length = torch.tensor(en_length).float()     \n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0\n",
    "    de_length = []\n",
    "    with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "        sentences_de = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            de_length.append(len(tok_sentence))\n",
    "            if len(tok_sentence) > de_max:\n",
    "                de_max = len(tok_sentence)\n",
    "                print(de_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_de.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_de.append(block_size*[100280])\n",
    "\n",
    "    de_length = torch.tensor(de_length).float()               \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "\n",
    "    #combine the arrays together\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    #check for indices in both sentences that have rows containing the DEL token\n",
    "    idx = np.where(sentences == 100280)\n",
    "\n",
    "    #delete every row that contains the DEL token\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = torch.tensor(sentences[0], dtype=torch.long)\n",
    "    sentences_de = torch.tensor(sentences[1], dtype=torch.long)\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(f\"Average length of english tokenized sentence: {torch.mean(en_length)}, and with std: {torch.std(en_length)}\")\n",
    "    print(f\"Average length of german tokenized sentence: {torch.mean(de_length)}, and with std: {torch.std(de_length)}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "create = False\n",
    "if create:\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND VAL DATASETS\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'rb') as f:\n",
    "    english_sentences = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'rb') as f:\n",
    "    german_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def PositionalEncoding(seq_len, n_embd):\n",
    "        \n",
    "    pos_enc = torch.zeros(seq_len, n_embd)\n",
    "    position = torch.arange(0, seq_len, dtype = torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, n_embd, 2) * (-math.log(10000.0) / n_embd))\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pos_enc.to(device)\n",
    "\n",
    "def get_padding_mask_matrix(x, x_embed):\n",
    "    \"\"\"\n",
    "    x is the (B, T) tokenized matrix with padding included\n",
    "    x_embed is the embedded matrix that we will convert all rows to zero based on the corresponding row index = padding index\n",
    "    \"\"\"\n",
    "\n",
    "    #locate every index in each tokenized sentence which contains the pad index\n",
    "    pad_indices = torch.nonzero(xb == 0).squeeze().to(device) #This will return a (N, 2) where the first column represents the sentence (batch_index) and the second column represents the corresponding index which is the pad index (This 2nd column represents which row we will set to all zeros)\n",
    "\n",
    "    #initialize a torch.ones of the shape of the embedding matrix\n",
    "    mask = torch.ones(x_embed.shape).to(device)\n",
    "\n",
    "    #For each row in the pad_indices matrix, we go to pad_indices[0] to grab the current batch example, and we go to the corresponding row of the batch example using the value of pad_indices[1]. We turn every column of that row into zeros\n",
    "    #ex: if the current row is [1, 4], then we go to the 2nd batch example, go to the 4th row, and wipe it clean with zeroes\n",
    "    mask[pad_indices[0], pad_indices[1], :] = 0\n",
    "\n",
    "    #element-wise product\n",
    "    x_padded = x_embed * mask\n",
    "\n",
    "    return x_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, decoder = False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        if self.decoder:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        #assume input is of size (B, T, C)\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "\n",
    "        if self.decoder:\n",
    "            attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "class crossHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.Wk = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wq = nn.Linear(n_embed, head_size, bias = False)\n",
    "        self.Wv = nn.Linear(n_embed, head_size, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_out):\n",
    "        B, T, C = x.shape\n",
    "        #assume x is of shape (B, T, C)\n",
    "        #assume enc_out is of shape (B, T, C)\n",
    "\n",
    "        K = self.Wk(enc_out) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(enc_out) #(B, T, head_size)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "        # print(\"attn\",attention_scores.shape)\n",
    "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(attention_scores, dim = -1) #(B, T, T)\n",
    "        scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = scores @ V #(B, T, head_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, head_size, decoder):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, decoder) for _ in range(n_heads)])\n",
    "        #output of heads is of size (B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(head_size * n_heads, n_embed)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([crossHead(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        x = torch.cat([h(x, enc_out) for h in self.heads], dim = -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_embed, 4*n_embed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4*n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = False)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x = self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderCrossBlock(nn.Module):\n",
    "    #one implementation of the multi head cross attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.heads = MultiHeadCrossAttention(head_size)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, parameters):\n",
    "        #assume parameters[0] is input of shape (B, T, C), It is the output of the decoder self attention layer\n",
    "        #assume parameters is a list of length 2: first element is the output of the previous hidden layer, and the 2nd element is the output of the encoder\n",
    "        \n",
    "        x = parameters[0]\n",
    "        enc_out = parameters[1]\n",
    "        \n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.heads(x, enc_out) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return [x, enc_out]\n",
    "\n",
    "class DecoderSelfBlock(nn.Module):\n",
    "    #one implementation of the multi head self attention block in the decoder\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_heads\n",
    "        self.sa = MultiHeadSelfAttention(head_size, decoder = True)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embed)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embed)\n",
    "        self.ffw = FeedForward()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.sa(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix_x = nn.Embedding(vocab_size, n_embed) * n_embed*(1/2)\n",
    "        #positional embedding is a function that requires no backpropagation, so we don't need to initialize it in here\n",
    "        self.tok_embedding_matrix_y = nn.Embedding(vocab_size, n_embed) * n_embed*(1/2)\n",
    "        \n",
    "        self.EncoderBlocks = nn.Sequential(*[EncoderBlock() for _ in range(n_layers)])\n",
    "        self.DecoderSelfBlocks = nn.Sequential(*[DecoderSelfBlock() for _ in range(n_layers)])\n",
    "        self.DecoderCrossBlocks = nn.Sequential(*[DecoderCrossBlock() for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
    "        self.final_linear = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    #looks to be very broken\n",
    "    def translate(self, x):\n",
    "        #assume input is a english sentence\n",
    "        tokenized_sentence = tokenizer.encode(x, allowed_special = specials)\n",
    "        #make sure the sentence is less than out block size\n",
    "        assert len(tokenized_sentence) <= block_size, print(\"this sentence is greater than our block_size\")\n",
    "\n",
    "        len_pad = block_size - len(tokenized_sentence)\n",
    "        tokenized_sentence = torch.tensor(tokenized_sentence + len_pad*[0]).view(1, -1).to(device)\n",
    "        run = True\n",
    "        input = [100278]\n",
    "        while run is True:\n",
    "            input = torch.tensor(input).view(1, -1).to(device)\n",
    "            T = input.shape[1]\n",
    "            logits, loss = self(tokenized_sentence[:,:T], input)\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            input = torch.cat((input, idx_next), dim = 1) #(B, T+1)\n",
    "\n",
    "            if idx_next == 100279:\n",
    "                run = False\n",
    "\n",
    "        return input[0].tolist()\n",
    "            \n",
    "    def forward(self, x, y, targets = None):\n",
    "        Bx, Tx = x.shape\n",
    "        Cx = n_embed\n",
    "        \n",
    "        tok_embed_x = self.tok_embedding_matrix_x(x)\n",
    "        pos_embed_x = PositionalEncoding(Tx, Cx)\n",
    "\n",
    "        By, Ty, = y.shape\n",
    "        Cy = n_embed\n",
    "\n",
    "        tok_embed_y = self.tok_embedding_matrix_y(y)\n",
    "        pos_embed_y = PositionalEncoding(Ty, Cy)\n",
    "\n",
    "        tok_pos_embed_x = tok_embed_x + pos_embed_x\n",
    "        tok_pos_embed_y = tok_embed_y + pos_embed_y\n",
    "        \n",
    "        masked_tok_embed_x = get_padding_mask_matrix(x, tok_pos_embed_x)\n",
    "        masked_tok_embed_y = get_padding_mask_matrix(y, tok_pos_embed_y)\n",
    "\n",
    "        x = masked_tok_embed_x\n",
    "        y = masked_tok_embed_y\n",
    "\n",
    "        #encoder\n",
    "        enc_out = self.EncoderBlocks(x)\n",
    "\n",
    "        #decoder self\n",
    "        y = self.DecoderSelfBlocks(y)\n",
    "        #decoder cross\n",
    "        #its ideal to send in one parameter only (i.e self, x) when passing parameters through stacked layers in an nn.Sequential, so we have to combine our previous hidden state output along with the enc_out into one object\n",
    "        y = self.DecoderCrossBlocks([y, enc_out])\n",
    "\n",
    "        #grab the transformed decoder input from the cross attention layer\n",
    "        y = y[0]\n",
    "        #remaining layers\n",
    "        y = self.final_layernorm(y)\n",
    "        logits = self.final_linear(y)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = logits.view(By*Ty, -1)\n",
    "            targets = targets.view(targets.shape[0]*targets.shape[1])\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.17081 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y, t = get_batch(split)\n",
    "            logits, loss = model(x, y, t)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.3025, val loss 2.3255\n",
      "step 500: train loss 2.2582, val loss 2.2900\n",
      "step 1000: train loss 2.2697, val loss 2.2266\n",
      "step 1500: train loss 2.2147, val loss 2.1929\n",
      "step 2000: train loss 2.1917, val loss 2.1668\n",
      "step 2500: train loss 2.1639, val loss 2.1969\n",
      "step 3000: train loss 2.1508, val loss 2.1401\n",
      "step 3500: train loss 2.1145, val loss 2.1128\n",
      "step 4000: train loss 2.0518, val loss 2.0902\n",
      "step 4500: train loss 2.0528, val loss 2.0786\n",
      "step 5000: train loss 2.0561, val loss 2.0532\n",
      "step 5500: train loss 2.0276, val loss 2.0301\n",
      "step 6000: train loss 2.0433, val loss 2.0288\n",
      "step 6500: train loss 1.9840, val loss 2.0237\n",
      "step 7000: train loss 1.9779, val loss 1.9638\n",
      "step 7500: train loss 1.9323, val loss 1.9767\n",
      "step 8000: train loss 1.9607, val loss 1.9611\n",
      "step 8500: train loss 1.9514, val loss 1.9296\n",
      "step 9000: train loss 1.9210, val loss 1.9312\n",
      "step 9500: train loss 1.9184, val loss 1.9175\n",
      "step 10000: train loss 1.8972, val loss 1.8785\n",
      "step 10500: train loss 1.8978, val loss 1.8984\n",
      "step 11000: train loss 1.8831, val loss 1.9006\n",
      "step 11500: train loss 1.8854, val loss 1.8576\n",
      "step 12000: train loss 1.8863, val loss 1.8624\n",
      "step 12500: train loss 1.8532, val loss 1.8363\n",
      "step 13000: train loss 1.8503, val loss 1.8421\n",
      "step 13500: train loss 1.8142, val loss 1.8016\n",
      "step 14000: train loss 1.8304, val loss 1.8325\n",
      "step 14500: train loss 1.8151, val loss 1.8294\n",
      "step 14999: train loss 1.8024, val loss 1.8016\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb, tb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb, tb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved at: d:\\Documents\\Github\\robots\\machine_model\\model.pt\n"
     ]
    }
   ],
   "source": [
    "filepath = os.getcwd()+\"\\\\machine_model\\\\model.pt\"\n",
    "torch.save(model.state_dict(), filepath)\n",
    "print(\"model saved at:\", filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "model.load_state_dict(torch.load(filepath))\n",
    "model.eval()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .'\n",
    "uh_oh = m.translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|START|> Das Ferienifer√∂sen allinten mit einem Restaurant Mitarbeiter mit den Platz mit allen Flughafen . <|END|>'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(uh_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 64 #This is the value of T\n",
    "batch_size = 16 #This it the value of B\n",
    "n_embed = 128\n",
    "dropout = 0.2\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (batch_size,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    #x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, t = get_batch('train')\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "x = tok_embedding_matrix(xb)\n",
    "y = tok_embedding_matrix(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Head(head_size = 64, decoder = False)\n",
    "e = MultiHeadSelfAttention(64, decoder = False)\n",
    "e = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_e = e(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Head(head_size = 64, decoder = True)\n",
    "#d = MultiHeadSelfAttention(head_size = 64, decoder = True)\n",
    "#d = DecoderSelfBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = crossHead(head_size = 64)\n",
    "c = MultiHeadCrossAttention(head_size = 64)\n",
    "c = DecoderCrossBlock()\n",
    "d = DecoderCrossBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat((out_e, out_d), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3266,  0.9439, -0.6072,  ..., -0.8942, -0.2462,  0.5541],\n",
       "         [ 0.3558, -0.0816, -1.0313,  ...,  0.2293, -0.4682,  1.1330],\n",
       "         [ 1.0091, -0.0226, -0.3073,  ..., -1.1772,  0.4133,  1.5344],\n",
       "         ...,\n",
       "         [ 0.7896,  0.3576, -0.5905,  ...,  0.0613, -0.5474,  0.6708],\n",
       "         [ 0.1137,  0.1101, -0.9164,  ...,  0.1007, -0.6001,  0.5187],\n",
       "         [ 0.6944,  0.8457, -0.8022,  ..., -0.2061, -0.3018,  0.6769]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(11.1942, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(xb, yb, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|PAD|>\", allowed_special = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoding.decode() got an unexpected keyword argument 'allowed_special'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[211], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39;49mdecode(english_sentences[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist(), allowed_special \u001b[39m=\u001b[39;49m specials)\n",
      "\u001b[1;31mTypeError\u001b[0m: Encoding.decode() got an unexpected keyword argument 'allowed_special'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(english_sentences[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ee0b04acb4cacacf37d05d30ef1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|PAD|> iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . <|PAD|>\n",
      "[0, 11245, 24532, 374, 264, 5644, 369, 1005, 25982, 902, 374, 17551, 439, 264, 1488, 1169, 555, 2231, 1919, 22145, 477, 14654, 304, 279, 51370, 13116, 320, 24359, 883, 315, 279, 9699, 6892, 354, 51370, 662, 220, 0]\n",
      "38\n",
      "38\n",
      "<|PAD|> iron cement protects the ingot against the hot , abrasive steel casting process . <|PAD|>\n",
      "[0, 11245, 24532, 36236, 279, 6892, 354, 2403, 279, 4106, 1174, 94804, 9699, 25146, 1920, 662, 220, 0]\n",
      "18\n",
      "<|PAD|> a fire restant repair cement for fire places , ovens , open fireplaces etc . <|PAD|>\n",
      "[0, 264, 4027, 2800, 519, 13023, 24532, 369, 4027, 7634, 1174, 297, 21778, 1174, 1825, 4027, 27170, 5099, 662, 220, 0]\n",
      "21\n",
      "<|PAD|> Construction and repair of highways and ... <|PAD|>\n",
      "[0, 24987, 323, 13023, 315, 60395, 323, 2564, 220, 0]\n",
      "10\n",
      "<|PAD|> An announcement must be commercial character . <|PAD|>\n",
      "[0, 1556, 17480, 2011, 387, 8518, 3752, 662, 220, 0]\n",
      "10\n",
      "38\n",
      "Length of sentences: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06345316f748eebf7f5a2b06fc3402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "Length of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "# block_size = 10\n",
    "# num_examples = 5\n",
    "\n",
    "# en_max = 0 \n",
    "# with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "#     idx_en = []\n",
    "#     sentences_en = []\n",
    "#     for i in tqdm(range(num_examples)):\n",
    "#         line = f.readline()\n",
    "#         line = line.replace(\"\\n\", \"\")\n",
    "#         len_pad = 0\n",
    "#         sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "#         print(sentence)\n",
    "#         print(tokenizer.encode(sentence, allowed_special = specials))\n",
    "#         tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "#         print(len(tok_sentence))\n",
    "#         if len(tok_sentence) > en_max:\n",
    "#             en_max = len(tok_sentence)\n",
    "#             print(en_max)\n",
    "\n",
    "#         if len(tok_sentence) <= block_size:\n",
    "#             len_pad = block_size - len(tok_sentence)\n",
    "#             tok_sentence = tok_sentence + len_pad*[100277]\n",
    "#             assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "#             #idx_en.append(i)\n",
    "#             sentences_en.append(tok_sentence)\n",
    "#         else:\n",
    "#             sentences_en.append(block_size*[100280])\n",
    "\n",
    "# print(en_max)    \n",
    "# print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "\n",
    "# de_max = 0 \n",
    "# with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "#     idx_de = []\n",
    "#     sentences_de = []\n",
    "#     for i in tqdm(range(num_examples)):\n",
    "#         line = f.readline()\n",
    "#         line = line.replace(\"\\n\", \"\")\n",
    "#         len_pad = 0\n",
    "#         sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "#         tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "#         if len(tok_sentence) > de_max:\n",
    "#             de_max = len(tok_sentence)\n",
    "#             print(de_max)\n",
    "\n",
    "#         if len(tok_sentence) <= block_size:\n",
    "#             len_pad = block_size - len(tok_sentence)\n",
    "#             tok_sentence = tok_sentence + len_pad*[100277]\n",
    "#             assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "#             #idx_en.append(i)\n",
    "#             sentences_de.append(tok_sentence)\n",
    "#         else:\n",
    "#             sentences_de.append(block_size*[100280])\n",
    "            \n",
    "# print(de_max)  \n",
    "# print(f\"Length of sentences: {len(sentences_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4\n",
      " 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "# sentences = np.array([sentences_en, sentences_de])\n",
    "\n",
    "# idx = np.where(sentences == 100280)\n",
    "# print(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = np.delete(sentences, idx[1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(2, 0, 10), dtype=int32)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_zero_indices(lst):\n",
    "    tensor = torch.tensor(lst)\n",
    "    zero_indices = torch.nonzero(tensor == 0).squeeze()\n",
    "    split_indices = torch.nonzero(torch.diff(zero_indices) != 1).squeeze() + 1\n",
    "    split_lists = torch.split(zero_indices, split_indices)\n",
    "\n",
    "    return [list(indices.numpy()) for indices in split_lists]\n",
    "\n",
    "# Example usage\n",
    "list1 = [[1, 2, 4, 0, 0, 0],[0, 4, 0, 2, 5, 8]]\n",
    "list2 = [0, 4, 0, 2, 5, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.tensor(list1)\n",
    "zero_indices = torch.nonzero(p == 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [0, 4],\n",
       "        [0, 5],\n",
       "        [1, 0],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = [3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(2, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_indices[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 0, 2])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_indices[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[zero_indices[:,0],zero_indices[:,1], :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    xdata = train_data_en if \"train\" else train_data_en\n",
    "    ydata = train_data_de if \"train\" else val_data_de\n",
    "    idx = torch.randint(len(xdata), (2,))\n",
    "    #print(idx)\n",
    "    x = torch.stack([xdata[i] for i in idx])\n",
    "    y = torch.stack([ydata[i] for i in idx])\n",
    "\n",
    "    #shifting our targets by 1 to the right\n",
    "    t = y[:, 1:]\n",
    "    #to pad the last dimension of the input tensor, pad has the form (padding_left, padding_right)\n",
    "    t = F.pad(input = t, pad = (0,1,0,0), mode = 'constant', value = 0)\n",
    "\n",
    "    #x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "    return x, y, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb, t = get_batch('train')\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embedding_matrix = nn.Embedding(vocab_size, 3)\n",
    "\n",
    "x = tok_embedding_matrix(xb)\n",
    "y = tok_embedding_matrix(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 45276,   916,   551, 32769, 17382, 84223,  4409,  1174, 84223,\n",
       "          1174, 59792,  1174,  3723, 15422,   482,   220,  2287, 27307,  8544,\n",
       "           662,   220,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [    0, 31672, 58427,   921, 97911, 75762,  1139,   279,  2217,  8863,\n",
       "          7620, 10487,   389,   701,  6500,   662,   578, 20206,  7860,   835,\n",
       "           567,    12,   567,   835,   567,   304,   374, 18641,   449, 29966,\n",
       "         45979,  1174, 45979, 35257,  1174,  9708,    75, 17646, 14355,  1322,\n",
       "          1174,  5099,   662,   220,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices = torch.nonzero(xb == 0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0, 22],\n",
       "        [ 0, 23],\n",
       "        [ 0, 24],\n",
       "        [ 0, 25],\n",
       "        [ 0, 26],\n",
       "        [ 0, 27],\n",
       "        [ 0, 28],\n",
       "        [ 0, 29],\n",
       "        [ 0, 30],\n",
       "        [ 0, 31],\n",
       "        [ 0, 32],\n",
       "        [ 0, 33],\n",
       "        [ 0, 34],\n",
       "        [ 0, 35],\n",
       "        [ 0, 36],\n",
       "        [ 0, 37],\n",
       "        [ 0, 38],\n",
       "        [ 0, 39],\n",
       "        [ 0, 40],\n",
       "        [ 0, 41],\n",
       "        [ 0, 42],\n",
       "        [ 0, 43],\n",
       "        [ 0, 44],\n",
       "        [ 0, 45],\n",
       "        [ 0, 46],\n",
       "        [ 0, 47],\n",
       "        [ 0, 48],\n",
       "        [ 0, 49],\n",
       "        [ 0, 50],\n",
       "        [ 0, 51],\n",
       "        [ 0, 52],\n",
       "        [ 0, 53],\n",
       "        [ 0, 54],\n",
       "        [ 0, 55],\n",
       "        [ 0, 56],\n",
       "        [ 0, 57],\n",
       "        [ 0, 58],\n",
       "        [ 0, 59],\n",
       "        [ 0, 60],\n",
       "        [ 0, 61],\n",
       "        [ 0, 62],\n",
       "        [ 0, 63],\n",
       "        [ 1,  0],\n",
       "        [ 1, 44],\n",
       "        [ 1, 45],\n",
       "        [ 1, 46],\n",
       "        [ 1, 47],\n",
       "        [ 1, 48],\n",
       "        [ 1, 49],\n",
       "        [ 1, 50],\n",
       "        [ 1, 51],\n",
       "        [ 1, 52],\n",
       "        [ 1, 53],\n",
       "        [ 1, 54],\n",
       "        [ 1, 55],\n",
       "        [ 1, 56],\n",
       "        [ 1, 57],\n",
       "        [ 1, 58],\n",
       "        [ 1, 59],\n",
       "        [ 1, 60],\n",
       "        [ 1, 61],\n",
       "        [ 1, 62],\n",
       "        [ 1, 63]])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = torch.ones(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad[zero_indices[:,0],zero_indices[:,1], :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3116,  0.0313, -0.2236],\n",
       "         [-0.8909, -0.7864,  0.5444],\n",
       "         [ 0.0336,  1.1090,  0.9984],\n",
       "         [ 0.6013,  0.4913,  0.5734],\n",
       "         [ 0.0890,  1.4799, -0.5040],\n",
       "         [ 0.4353, -1.7232, -1.2105],\n",
       "         [ 1.6703, -1.3551, -0.1944],\n",
       "         [-0.2170,  0.1159,  0.5018],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 1.6703, -1.3551, -0.1944],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [-1.0707, -1.5301,  0.3355],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 1.0630, -1.0203, -0.1490],\n",
       "         [-1.1775,  1.1844,  0.0845],\n",
       "         [-0.0749,  1.0951, -0.5779],\n",
       "         [-1.9409, -1.5697, -0.8750],\n",
       "         [ 1.0254,  0.7691,  1.1471],\n",
       "         [ 0.3585, -0.5508, -1.4044],\n",
       "         [-0.9088, -0.8208, -1.3070],\n",
       "         [-0.6819, -1.0715, -0.7244],\n",
       "         [-1.9409, -1.5697, -0.8750],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236]],\n",
       "\n",
       "        [[-1.3116,  0.0313, -0.2236],\n",
       "         [-0.6757, -0.1480, -1.2202],\n",
       "         [-1.2214,  0.2600,  0.1718],\n",
       "         [ 0.3620,  0.6951, -1.2541],\n",
       "         [ 0.4324,  0.4025, -0.1587],\n",
       "         [ 0.3028,  2.2365,  0.3031],\n",
       "         [-1.8006, -1.4027, -0.7702],\n",
       "         [ 0.4732, -1.2912,  0.6969],\n",
       "         [-2.5134, -0.3764, -0.7584],\n",
       "         [ 1.4576,  0.0583, -1.1155],\n",
       "         [ 0.3161,  0.8686, -0.8626],\n",
       "         [ 0.5494,  0.0795, -2.3261],\n",
       "         [-0.8432,  0.4319,  1.5007],\n",
       "         [ 2.3933, -0.0890,  0.3930],\n",
       "         [ 0.0756,  1.3468, -0.6566],\n",
       "         [-0.6819, -1.0715, -0.7244],\n",
       "         [ 0.1710, -1.3883,  0.8507],\n",
       "         [ 0.1043,  1.0438,  0.4095],\n",
       "         [ 0.0896, -1.7408, -0.4866],\n",
       "         [ 0.6601,  1.2032, -1.1852],\n",
       "         [-0.4363,  0.3361,  0.7826],\n",
       "         [ 0.8432, -1.0301, -1.1662],\n",
       "         [-0.4363,  0.3361,  0.7826],\n",
       "         [ 0.6601,  1.2032, -1.1852],\n",
       "         [-0.4363,  0.3361,  0.7826],\n",
       "         [ 0.3573,  0.6830, -0.9094],\n",
       "         [ 1.6946,  0.2821, -0.2654],\n",
       "         [-0.9574, -0.0107, -0.4854],\n",
       "         [ 2.3888,  1.7909,  0.5773],\n",
       "         [-1.4190,  1.1800,  0.2859],\n",
       "         [ 0.1694,  0.6371, -1.3927],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 0.1694,  0.6371, -1.3927],\n",
       "         [ 0.9327,  2.7646, -1.1245],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 1.3364, -0.3491,  0.9977],\n",
       "         [-0.0504, -2.0265,  1.5968],\n",
       "         [ 0.0190, -1.0524,  0.4220],\n",
       "         [ 1.3054,  0.2281, -0.0085],\n",
       "         [-0.0486, -0.2269, -0.5544],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 0.7936, -1.9172, -0.7717],\n",
       "         [-0.6819, -1.0715, -0.7244],\n",
       "         [-1.9409, -1.5697, -0.8750],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236],\n",
       "         [-1.3116,  0.0313, -0.2236]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded = x * pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000,  0.0000, -0.0000],\n",
       "         [-0.8909, -0.7864,  0.5444],\n",
       "         [ 0.0336,  1.1090,  0.9984],\n",
       "         [ 0.6013,  0.4913,  0.5734],\n",
       "         [ 0.0890,  1.4799, -0.5040],\n",
       "         [ 0.4353, -1.7232, -1.2105],\n",
       "         [ 1.6703, -1.3551, -0.1944],\n",
       "         [-0.2170,  0.1159,  0.5018],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 1.6703, -1.3551, -0.1944],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [-1.0707, -1.5301,  0.3355],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 1.0630, -1.0203, -0.1490],\n",
       "         [-1.1775,  1.1844,  0.0845],\n",
       "         [-0.0749,  1.0951, -0.5779],\n",
       "         [-1.9409, -1.5697, -0.8750],\n",
       "         [ 1.0254,  0.7691,  1.1471],\n",
       "         [ 0.3585, -0.5508, -1.4044],\n",
       "         [-0.9088, -0.8208, -1.3070],\n",
       "         [-0.6819, -1.0715, -0.7244],\n",
       "         [-1.9409, -1.5697, -0.8750],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.0000,  0.0000, -0.0000],\n",
       "         [-0.6757, -0.1480, -1.2202],\n",
       "         [-1.2214,  0.2600,  0.1718],\n",
       "         [ 0.3620,  0.6951, -1.2541],\n",
       "         [ 0.4324,  0.4025, -0.1587],\n",
       "         [ 0.3028,  2.2365,  0.3031],\n",
       "         [-1.8006, -1.4027, -0.7702],\n",
       "         [ 0.4732, -1.2912,  0.6969],\n",
       "         [-2.5134, -0.3764, -0.7584],\n",
       "         [ 1.4576,  0.0583, -1.1155],\n",
       "         [ 0.3161,  0.8686, -0.8626],\n",
       "         [ 0.5494,  0.0795, -2.3261],\n",
       "         [-0.8432,  0.4319,  1.5007],\n",
       "         [ 2.3933, -0.0890,  0.3930],\n",
       "         [ 0.0756,  1.3468, -0.6566],\n",
       "         [-0.6819, -1.0715, -0.7244],\n",
       "         [ 0.1710, -1.3883,  0.8507],\n",
       "         [ 0.1043,  1.0438,  0.4095],\n",
       "         [ 0.0896, -1.7408, -0.4866],\n",
       "         [ 0.6601,  1.2032, -1.1852],\n",
       "         [-0.4363,  0.3361,  0.7826],\n",
       "         [ 0.8432, -1.0301, -1.1662],\n",
       "         [-0.4363,  0.3361,  0.7826],\n",
       "         [ 0.6601,  1.2032, -1.1852],\n",
       "         [-0.4363,  0.3361,  0.7826],\n",
       "         [ 0.3573,  0.6830, -0.9094],\n",
       "         [ 1.6946,  0.2821, -0.2654],\n",
       "         [-0.9574, -0.0107, -0.4854],\n",
       "         [ 2.3888,  1.7909,  0.5773],\n",
       "         [-1.4190,  1.1800,  0.2859],\n",
       "         [ 0.1694,  0.6371, -1.3927],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 0.1694,  0.6371, -1.3927],\n",
       "         [ 0.9327,  2.7646, -1.1245],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 1.3364, -0.3491,  0.9977],\n",
       "         [-0.0504, -2.0265,  1.5968],\n",
       "         [ 0.0190, -1.0524,  0.4220],\n",
       "         [ 1.3054,  0.2281, -0.0085],\n",
       "         [-0.0486, -0.2269, -0.5544],\n",
       "         [ 0.2051, -3.0432, -0.6218],\n",
       "         [ 0.7936, -1.9172, -0.7717],\n",
       "         [-0.6819, -1.0715, -0.7244],\n",
       "         [-1.9409, -1.5697, -0.8750],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000,  0.0000, -0.0000]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
