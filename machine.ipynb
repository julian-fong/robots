{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "block_size = 256 #This is the value of T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100282\n",
      "{'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276, '<|PAD|>': 0, '<|START|>': 100278, '<|END|>': 100279, '<|DEL|>': 100280, '!': 100281}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#dataset https://nlp.stanford.edu/projects/nmt/\n",
    "\n",
    "#tiktoken api https://github.com/openai/tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|PAD|>\": 0,\n",
    "        \"<|START|>\": 100278,\n",
    "        \"<|END|>\": 100279,\n",
    "        \"<|DEL|>\": 100280,\n",
    "        \"!\": 100281\n",
    "    }\n",
    ")\n",
    "print(tokenizer.n_vocab) #this is the number of tokens in our tokenizer\n",
    "print(tokenizer._special_tokens) #prints out our special tokens \n",
    "\n",
    "specials = {\"<|PAD|>\",\"<|START|>\",\"<|END|>\", \"<|DEL|>\", \"!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    num_examples = 100000\n",
    "\n",
    "    en_max = 0 \n",
    "    with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "        idx_en = []\n",
    "        sentences_en = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            if len(tok_sentence) > en_max:\n",
    "                en_max = len(tok_sentence)\n",
    "                print(en_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_en.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_en.append(block_size*[100280])\n",
    "\n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    print(en_max)    \n",
    "    print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "    de_max = 0 \n",
    "    with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "        idx_de = []\n",
    "        sentences_de = []\n",
    "        for i in tqdm(range(num_examples)):\n",
    "            line = f.readline()\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            len_pad = 0\n",
    "            sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "            tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "            if len(tok_sentence) > de_max:\n",
    "                de_max = len(tok_sentence)\n",
    "                print(de_max)\n",
    "\n",
    "            if len(tok_sentence) <= block_size:\n",
    "                len_pad = block_size - len(tok_sentence)\n",
    "                tok_sentence = tok_sentence + len_pad*[0]\n",
    "                assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "                sentences_de.append(tok_sentence)\n",
    "            else:\n",
    "                sentences_de.append(block_size*[100280])\n",
    "                \n",
    "    print(de_max) \n",
    "    print(f\"Length of sentences: {len(sentences_de)}\")\n",
    "\n",
    "    print(\"Removing sentences whos length is greater than our block_size\")\n",
    "    sentences = np.array([sentences_en, sentences_de])\n",
    "    idx = np.where(sentences == 100280)\n",
    "    sentences = np.delete(sentences, idx[1], axis = 1)\n",
    "\n",
    "    #splitting to german and english\n",
    "\n",
    "    sentences_en = sentences[0]\n",
    "    sentences_de = sentences[1]\n",
    "\n",
    "    print(f\"Length of new english sentences: {len(sentences_en)}\")\n",
    "    print(f\"Length of new german sentences: {len(sentences_de)}\")\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_en, f)\n",
    "\n",
    "    with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'wb') as f:\n",
    "        pickle.dump(sentences_de, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c8eb00192e4a61bf77e9fe8821bc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "50\n",
      "60\n",
      "67\n",
      "68\n",
      "82\n",
      "95\n",
      "122\n",
      "125\n",
      "132\n",
      "141\n",
      "154\n",
      "177\n",
      "191\n",
      "203\n",
      "295\n",
      "295\n",
      "Length of sentences: 100000\n",
      "295\n",
      "Length of sentences: 100000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fa2302ddb14555a8860e9a79ffd353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "73\n",
      "83\n",
      "91\n",
      "105\n",
      "120\n",
      "130\n",
      "135\n",
      "161\n",
      "163\n",
      "187\n",
      "239\n",
      "267\n",
      "284\n",
      "284\n",
      "Length of sentences: 100000\n",
      "Length of new english sentences: 99996\n",
      "Length of new german sentences: 99996\n"
     ]
    }
   ],
   "source": [
    "create = True\n",
    "if create:\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN AND VAL DATASETS\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\english_sentences.pkl', 'rb') as f:\n",
    "    english_sentences = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd()+'\\\\data\\\\german_sentences.pkl', 'rb') as f:\n",
    "    german_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100280 in english_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99996, 256)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 11245, 24532,   374,   264,  5644,   369,  1005, 25982,\n",
       "         902,   374, 17551,   439,   264,  1488,  1169,   555,  2231,\n",
       "        1919, 22145,   477, 14654,   304,   279, 51370, 13116,   320,\n",
       "       24359,   883,   315,   279,  9699,  6892,   354, 51370,   662,\n",
       "         220,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH LOADER\n",
    "n = int(0.9*len(english_sentences))\n",
    "\n",
    "train_data_en = english_sentences[:n]\n",
    "val_data_en = english_sentences[n:]\n",
    "\n",
    "train_data_de = german_sentences[:n]\n",
    "val_data_de = german_sentences[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|PAD|>\", allowed_special = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ee0b04acb4cacacf37d05d30ef1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|PAD|> iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . <|PAD|>\n",
      "[0, 11245, 24532, 374, 264, 5644, 369, 1005, 25982, 902, 374, 17551, 439, 264, 1488, 1169, 555, 2231, 1919, 22145, 477, 14654, 304, 279, 51370, 13116, 320, 24359, 883, 315, 279, 9699, 6892, 354, 51370, 662, 220, 0]\n",
      "38\n",
      "38\n",
      "<|PAD|> iron cement protects the ingot against the hot , abrasive steel casting process . <|PAD|>\n",
      "[0, 11245, 24532, 36236, 279, 6892, 354, 2403, 279, 4106, 1174, 94804, 9699, 25146, 1920, 662, 220, 0]\n",
      "18\n",
      "<|PAD|> a fire restant repair cement for fire places , ovens , open fireplaces etc . <|PAD|>\n",
      "[0, 264, 4027, 2800, 519, 13023, 24532, 369, 4027, 7634, 1174, 297, 21778, 1174, 1825, 4027, 27170, 5099, 662, 220, 0]\n",
      "21\n",
      "<|PAD|> Construction and repair of highways and ... <|PAD|>\n",
      "[0, 24987, 323, 13023, 315, 60395, 323, 2564, 220, 0]\n",
      "10\n",
      "<|PAD|> An announcement must be commercial character . <|PAD|>\n",
      "[0, 1556, 17480, 2011, 387, 8518, 3752, 662, 220, 0]\n",
      "10\n",
      "38\n",
      "Length of sentences: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c06345316f748eebf7f5a2b06fc3402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "Length of sentences: 5\n"
     ]
    }
   ],
   "source": [
    "block_size = 10\n",
    "num_examples = 5\n",
    "\n",
    "en_max = 0 \n",
    "with open(os.getcwd()+'\\\\data\\\\train_en.txt', 'r', encoding='utf8') as f:\n",
    "    idx_en = []\n",
    "    sentences_en = []\n",
    "    for i in tqdm(range(num_examples)):\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|PAD|> \" + (line) + \" <|PAD|>\"\n",
    "        print(sentence)\n",
    "        print(tokenizer.encode(sentence, allowed_special = specials))\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        print(len(tok_sentence))\n",
    "        if len(tok_sentence) > en_max:\n",
    "            en_max = len(tok_sentence)\n",
    "            print(en_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[100277]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            #idx_en.append(i)\n",
    "            sentences_en.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_en.append(block_size*[100280])\n",
    "\n",
    "print(en_max)    \n",
    "print(f\"Length of sentences: {len(sentences_en)}\")\n",
    "\n",
    "\n",
    "de_max = 0 \n",
    "with open(os.getcwd()+'\\\\data\\\\train_de.txt', 'r', encoding='utf8') as f:\n",
    "    idx_de = []\n",
    "    sentences_de = []\n",
    "    for i in tqdm(range(num_examples)):\n",
    "        line = f.readline()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        len_pad = 0\n",
    "        sentence = \"<|START|> \" + (line) + \" <|END|>\"\n",
    "        tok_sentence = tokenizer.encode(sentence, allowed_special = specials)\n",
    "        if len(tok_sentence) > de_max:\n",
    "            de_max = len(tok_sentence)\n",
    "            print(de_max)\n",
    "\n",
    "        if len(tok_sentence) <= block_size:\n",
    "            len_pad = block_size - len(tok_sentence)\n",
    "            tok_sentence = tok_sentence + len_pad*[100277]\n",
    "            assert len(tok_sentence) == block_size, print(len(tok_sentence))\n",
    "            #idx_en.append(i)\n",
    "            sentences_de.append(tok_sentence)\n",
    "        else:\n",
    "            sentences_de.append(block_size*[100280])\n",
    "            \n",
    "print(de_max)  \n",
    "print(f\"Length of sentences: {len(sentences_de)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sentences_en).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array([sentences_en, sentences_de])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4\n",
      " 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "sentences = np.array([sentences_en, sentences_de])\n",
    "\n",
    "idx = np.where(sentences == 100280)\n",
    "print(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.delete(sentences, idx[1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(2, 0, 10), dtype=int32)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
