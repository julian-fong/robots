{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+'\\\\data\\\\input.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "#strategy to tokenize text\n",
    "\n",
    "#character level mapping\n",
    "\n",
    "s_to_i = {ch:i for i,ch in enumerate(chars)}\n",
    "i_to_s = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [s_to_i[c] for c in s]\n",
    "decode = lambda l: ''.join([i_to_s[c] for c in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "#take all of the text and encode them into pytorch tensors\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into a train and validation split\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the text sequences into the transformer\n",
    "\n",
    "#we only work with chunks of the dataset. when we train the model, we sample chunks of the dataset and train on just chunks at a time. chunks have a maximum length: called block_size\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "\n",
    "#shift to the right by 1 to get the target\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"when the input is {context}, the target will be {target}\")\n",
    "\n",
    "#we want the model to be robust when seeing contexts of multiple lengths, we set it as 8 so that the model has the capability of looking back at least 'block_size' characters of contexts of. \n",
    "# When we need to predict the 9th or 10th character, we start to truncate the context to continue onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "batch_size = 64 #This is the value of B\n",
    "block_size = 256 #This is the value of T\n",
    "n_embd = 512 #This is the value of C\n",
    "n_head = 8\n",
    "dropout = 0.25\n",
    "n_layers = 8 #number of decoder blocks we will initialize\n",
    "max_new_tokens = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the batches\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y, = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# print('inputs:')\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print('targets:')\n",
    "# print(yb.shape)\n",
    "# print(yb)\n",
    "\n",
    "#xb, yb, of shape (B, T) where B = batch_size, and T = block_size (time sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer time!\n",
    "\n",
    "head -> multi-head self attention -> feedforward -> block -> Decoder\n",
    "\n",
    "special layers:\n",
    "dropout\n",
    "layernorm\n",
    "linear\n",
    "embedding\n",
    "pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch's positional encoding https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            x: (T, B, C)\n",
    "            We have to change our shape dimensions in to (T, B, C) and then change it back to (B, T, C) when done\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One Head of self-attention \n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "        #initialize the key, query, and value matrices\n",
    "        self.Wk = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.Wq = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.Wv = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        #since this is a decoder, we need to initialize the mask as well\n",
    "\n",
    "        #we register this as a buffer, which still exists as a 'matrix' to use, but we don't compute gradients on this or use in the backward pass\n",
    "        #model parameters are objects that we use during the forward pass and we update using gradient descent\n",
    "        #model buffers are objects that we use during computation but do not update\n",
    "\n",
    "        #both parameters and buffers are saved to the right device when calling .to_device\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        #K.T needs to be of shape (B, C, T), so we swap the -2 and -1 positions\n",
    "        scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**(1/2) #(B, T, T)\n",
    "        masked_scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T), turn from 0 to -inf\n",
    "        attention_scores = F.softmax(masked_scores, dim = -1) #applying softmax along the rows (B, T, T)\n",
    "        attention_scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = attention_scores @ V #(B, T, head_size)\n",
    "        \n",
    "        return out #(B, T, head_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) #(B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(n_heads * head_size, n_embd) #paper specifies a final linear layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "\n",
    "        #Each Head returns a output of size (B, T, head_size), we concatenate along the final dimension so that our variable 'out' is now (B, T, n_heads*head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) #(B, T, n_heads*head_size)\n",
    "        out = self.proj(out) #(B, T, C)\n",
    "        out = self.dropout(out) #(B, T, C)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #input and output will be of size (B, T, C)\n",
    "        self.ff1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.ff2 = nn.Linear(4*n_embd, n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.ff1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ff2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    #implementaion of one transformer block\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embd // n_head\n",
    "\n",
    "        self.sa = MultiHeadAttention(n_head, self.head_size)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embd)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input of x is size (B, T, C) where x is the sum of the embedded input + positional_encoding\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = PositionalEncoding(n_embd)\n",
    "\n",
    "        # need '*' before list comprehension otherwise we get TypeError: list is not a Module subclass\n",
    "        self.blocks = nn.Sequential(*[Block(n_head) for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, x, y = None):\n",
    "        B, T = x.shape\n",
    "        C = n_embd\n",
    "        #assume our inputs x are of size (B, T)\n",
    "        #assume our targets y are of size (B)\n",
    "\n",
    "        token_embed = self.tok_embedding_matrix(x) #(B, T, C)\n",
    "        pos_embed = self.pos_embedding(token_embed.view(T,B,C)).view(B, T, C) #(B, T, C)\n",
    "\n",
    "        input = token_embed + pos_embed #(B, T, C)\n",
    "        input = self.blocks(input) #(B, T, C)\n",
    "\n",
    "        input = self.final_layer_norm(input) #(B, T, C)\n",
    "        logits = self.final_linear(input) #(B, T, C)\n",
    "\n",
    "        if y is not None:\n",
    "            logits = logits.view(B*T, -1) #(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "    #generating some stuff\n",
    "    #idx is (B, T) array of indices in our current context <-- current context of some list of characters in some batch\n",
    "    #we keep extending (B, T) to (B, T+1), (B, T+2) and so on.. continuing until we reach max new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx_cond) #<-- output of this is (B, T, C)\n",
    "            #print(f\"new dim of logits: {logits.shape}\")\n",
    "            #focus only on the last time step because the last time step is the prediction on what comes next\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the stuff\n",
    "\n",
    "model = Decoder()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to estimate the loss every once in a while\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "# xb = tokens(xb)\n",
    "B, T = xb.shape\n",
    "print(xb.shape)\n",
    "xb = xb.view(T,B)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = PositionalEncoding(32)\n",
    "B, T, C = xb.shape\n",
    "pos(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H = MultiHeadAttention(2, 16)\n",
    "#H = Head(16)\n",
    "H = Block(2)\n",
    "H(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
