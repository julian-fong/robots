{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+'\\\\data\\\\input.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n",
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "#strategy to tokenize text\n",
    "\n",
    "#character level mapping\n",
    "\n",
    "s_to_i = {ch:i for i,ch in enumerate(chars)}\n",
    "i_to_s = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [s_to_i[c] for c in s]\n",
    "decode = lambda l: ''.join([i_to_s[c] for c in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "#take all of the text and encode them into pytorch tensors\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into a train and validation split\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input the text sequences into the transformer\n",
    "\n",
    "#we only work with chunks of the dataset. when we train the model, we sample chunks of the dataset and train on just chunks at a time. chunks have a maximum length: called block_size\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "\n",
    "#shift to the right by 1 to get the target\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the input is tensor([18]), the target will be 47\n",
      "when the input is tensor([18, 47]), the target will be 56\n",
      "when the input is tensor([18, 47, 56]), the target will be 57\n",
      "when the input is tensor([18, 47, 56, 57]), the target will be 58\n",
      "when the input is tensor([18, 47, 56, 57, 58]), the target will be 1\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1]), the target will be 15\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15]), the target will be 47\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target will be 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"when the input is {context}, the target will be {target}\")\n",
    "\n",
    "#we want the model to be robust when seeing contexts of multiple lengths, we set it as 8 so that the model has the capability of looking back at least 'block_size' characters of contexts of. \n",
    "# When we need to predict the 9th or 10th character, we start to truncate the context to continue onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "batch_size = 4 #This is the value of B\n",
    "block_size = 8 #This is the value of T\n",
    "n_embd = 32 #This is the value of C\n",
    "dropout = 0.4\n",
    "n_layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "#getting the batches\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "# print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "#xb, yb, of shape (B, T) where B = batch_size, and T = block_size (time sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer time!\n",
    "\n",
    "head -> multi-head self attention -> feedforward -> block -> Decoder\n",
    "\n",
    "special layers:\n",
    "dropout\n",
    "layernorm\n",
    "linear\n",
    "embedding\n",
    "pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One Head of self-attention \n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "        #initialize the key, query, and value matrices\n",
    "        self.Wk = nn.Linear(n_embd, head_size)\n",
    "        self.Wq = nn.Linear(n_embd, head_size)\n",
    "        self.Wv = nn.Linear(n_embd, head_size)\n",
    "\n",
    "        #since this is a decoder, we need to initialize the mask as well\n",
    "\n",
    "        #we register this as a buffer, which still exists as a 'matrix' to use, but we don't compute gradients on this or use in the backward pass\n",
    "        #model parameters are objects that we use during the forward pass and we update using gradient descent\n",
    "        #model buffers are objects that we use during computation but do not update\n",
    "\n",
    "        #both parameters and buffers are saved to the right device when calling .to_device\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        #K.T needs to be of shape (B, C, T), so we swap the -2 and -1 positions\n",
    "        scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**2 #(B, T, T)\n",
    "        masked_scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(masked_scores, dim = -1) #applying softmax along the rows (B, T, T)\n",
    "        attention_scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = attention_scores @ V #(B, T, head_size)\n",
    "        \n",
    "        return out #(B, T, head_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) #(B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(n_heads * head_size, n_embd) #paper specifies a final linear layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "\n",
    "        #Each Head returns a output of size (B, T, head_size), we concatenate along the final dimension so that our variable 'out' is now (B, T, n_heads*head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) #(B, T, n_heads*head_size)\n",
    "        out = self.proj(out) #(B, T, C)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        #input and output will be of size (B, T, C)\n",
    "        self.ff1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.ff2 = nn.Linear(4*n_embd, n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.ff1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ff2(x)\n",
    "        x = dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    #implementaion of one transformer block\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embd / n_head\n",
    "\n",
    "        self.sa = MultiHeadAttention(n_head, self.head_size)\n",
    "        self.ffw = FeedForward(n_embd)\n",
    "        self.layernorm1 = nn.LayerNorm(n_embd)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input of x is size (B, T, C) where x is the sum of the embedded input + positional_encoding\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_blocks, n_heads):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.blocks = nn.Sequential([Block(n_heads) for _ in n_blocks])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        B, T, C, = x.shape\n",
    "        #assume our inputs x are of size (B, T)\n",
    "        #assume our targets y are of size (B)\n",
    "        token_embed = self.tok_embedding_matrix(x)\n",
    "        pos_embed = self.pos_embedding(x)\n",
    "\n",
    "        input = token_embed + pos_embed\n",
    "        input = self.blocks(input)\n",
    "\n",
    "        input = self.final_layer_norm(input)\n",
    "        logits = self.final_linear(input)\n",
    "\n",
    "        logits = logits.view(B*T, -1)\n",
    "\n",
    "        if y:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "xb = tokens(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5003, 0.4997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3348, 0.3322, 0.3330, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2492, 0.2510, 0.2496, 0.2503, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1991, 0.1999, 0.2016, 0.2005, 0.1989, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1666, 0.1664, 0.1666, 0.1663, 0.1660, 0.1681, 0.0000, 0.0000],\n",
      "        [0.1434, 0.1436, 0.1420, 0.1428, 0.1433, 0.1431, 0.1419, 0.0000],\n",
      "        [0.1251, 0.1250, 0.1250, 0.1247, 0.1249, 0.1253, 0.1249, 0.1250]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0273,  0.9228,  0.0360,  0.1456,  0.6187, -0.0846, -1.0352,  0.0662,\n",
      "          0.2726, -0.2818,  1.1334,  0.2416, -0.7980, -0.2744, -0.6682, -0.8704],\n",
      "        [-0.2197,  0.0495, -0.1741,  0.7288,  0.2270,  0.3235,  0.9681,  0.8418,\n",
      "          1.7442, -0.3903, -0.4830, -0.3293,  0.0269,  0.3564, -0.5448,  0.5546],\n",
      "        [ 0.5079,  0.9698, -0.8998, -0.5712, -0.3823, -0.2717,  0.9636,  0.5586,\n",
      "          0.3105, -0.2019, -0.4547, -1.3811,  0.9210,  0.0294, -0.3458,  0.8079],\n",
      "        [ 0.6247, -0.4216, -0.6158,  0.3100, -0.1949,  0.3252,  0.6980, -0.3477,\n",
      "          0.7321,  0.6415,  0.9923, -0.6636, -0.6263,  0.8782, -0.5015, -0.2136],\n",
      "        [-0.1783,  0.5257, -0.8681,  0.0170,  0.4809,  0.6153,  0.6748, -0.0405,\n",
      "         -0.0358, -0.2840,  0.8452, -0.2903,  0.0398,  1.1790, -0.3354, -0.1513],\n",
      "        [-0.7823,  0.4361, -0.2712, -0.0683,  0.9084,  0.2375, -0.0257, -0.1244,\n",
      "         -0.4127,  0.5316,  0.7339, -0.2951,  0.2253,  1.2894,  0.3514,  0.8836],\n",
      "        [-0.2255, -0.2212, -0.0234,  0.7066, -0.1885,  0.0650,  0.6474,  1.2408,\n",
      "          1.8600, -0.4701, -0.4893, -0.3005,  0.3373, -0.0857,  0.0273,  0.7268],\n",
      "        [-0.2197,  0.0495, -0.1741,  0.7288,  0.2270,  0.3235,  0.9681,  0.8418,\n",
      "          1.7442, -0.3903, -0.4830, -0.3293,  0.0269,  0.3564, -0.5448,  0.5546]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.7323e-02,  9.2276e-01,  3.6048e-02,  1.4556e-01,  6.1869e-01,\n",
       "          -8.4573e-02, -1.0352e+00,  6.6203e-02,  2.7262e-01, -2.8181e-01,\n",
       "           1.1334e+00,  2.4156e-01, -7.9797e-01, -2.7442e-01, -6.6818e-01,\n",
       "          -8.7045e-01],\n",
       "         [-1.2348e-01,  4.8636e-01, -6.8990e-02,  4.3702e-01,  4.2295e-01,\n",
       "           1.1937e-01, -3.4105e-02,  4.5382e-01,  1.0080e+00, -3.3605e-01,\n",
       "           3.2560e-01, -4.3730e-02, -3.8574e-01,  4.0809e-02, -6.0651e-01,\n",
       "          -1.5832e-01],\n",
       "         [ 8.6963e-02,  6.4830e-01, -3.4540e-01,  1.0065e-01,  1.5526e-01,\n",
       "          -1.1321e-02,  2.9588e-01,  4.8785e-01,  7.7409e-01, -2.9125e-01,\n",
       "           6.7575e-02, -4.8841e-01,  4.8466e-02,  3.6290e-02, -5.1983e-01,\n",
       "           1.6180e-01],\n",
       "         [ 2.2114e-01,  3.7889e-01, -4.1342e-01,  1.5419e-01,  6.6952e-02,\n",
       "           7.3688e-02,  4.0018e-01,  2.8017e-01,  7.6637e-01, -5.8014e-02,\n",
       "           2.9604e-01, -5.3323e-01, -1.1896e-01,  2.4816e-01, -5.1502e-01,\n",
       "           7.0441e-02],\n",
       "         [ 1.4276e-01,  4.0917e-01, -5.0512e-01,  1.2508e-01,  1.4811e-01,\n",
       "           1.8063e-01,  4.5578e-01,  2.1636e-01,  6.0525e-01, -1.0274e-01,\n",
       "           4.0447e-01, -4.8690e-01, -8.5500e-02,  4.3307e-01, -4.7892e-01,\n",
       "           2.7476e-02],\n",
       "         [-1.3738e-02,  4.1392e-01, -4.6497e-01,  9.3279e-02,  2.7729e-01,\n",
       "           1.9064e-01,  3.7296e-01,  1.5869e-01,  4.3382e-01,  3.4366e-03,\n",
       "           4.6140e-01, -4.5280e-01, -3.4716e-02,  5.7706e-01, -3.3956e-01,\n",
       "           1.6978e-01],\n",
       "         [-4.3675e-02,  3.2327e-01, -4.0203e-01,  1.8157e-01,  2.1136e-01,\n",
       "           1.7355e-01,  4.1191e-01,  3.1250e-01,  6.3781e-01, -6.4839e-02,\n",
       "           3.2697e-01, -4.2996e-01,  1.6631e-02,  4.8269e-01, -2.8860e-01,\n",
       "           2.4694e-01],\n",
       "         [-6.5472e-02,  2.8920e-01, -3.7361e-01,  2.4950e-01,  2.1244e-01,\n",
       "           1.9161e-01,  4.8192e-01,  3.7963e-01,  7.7646e-01, -1.0569e-01,\n",
       "           2.2432e-01, -4.1829e-01,  1.9295e-02,  4.6606e-01, -3.2000e-01,\n",
       "           2.8676e-01]],\n",
       "\n",
       "        [[-6.0159e-02, -6.4548e-02, -1.8557e-01,  9.8290e-02, -7.7030e-01,\n",
       "          -1.6189e-01,  1.7985e-01,  2.0814e-01, -5.3978e-01,  3.9940e-03,\n",
       "           4.3372e-01, -5.0999e-02, -8.1254e-01,  6.0069e-01,  1.5512e-01,\n",
       "           3.3682e-01],\n",
       "         [-2.9760e-01,  2.3248e-01, -1.4343e-01, -1.6064e-01, -5.9804e-01,\n",
       "           2.2227e-01, -2.9902e-01, -2.6042e-01, -7.0827e-01,  9.2529e-02,\n",
       "           3.3341e-01, -3.7657e-01, -1.8829e-01,  2.6511e-01,  6.2945e-02,\n",
       "           5.8198e-01],\n",
       "         [-8.6276e-02,  4.0395e-02, -6.5211e-02, -2.0766e-01, -3.6308e-01,\n",
       "          -1.7704e-01, -1.0419e-01, -1.7404e-01, -4.3748e-01,  2.3380e-01,\n",
       "           4.2333e-01, -3.6063e-01,  1.8744e-01,  3.2668e-01,  5.3852e-02,\n",
       "           5.3809e-01],\n",
       "         [-2.6100e-01,  1.4041e-01, -1.1639e-01, -1.7409e-01, -4.4607e-02,\n",
       "          -7.2077e-02, -8.6636e-02, -1.6373e-01, -4.3184e-01,  3.0869e-01,\n",
       "           5.0048e-01, -3.4576e-01,  2.0010e-01,  5.6558e-01,  1.2770e-01,\n",
       "           6.2547e-01],\n",
       "         [-1.0662e-01,  3.0459e-01, -2.7314e-01, -2.5183e-01, -1.1256e-01,\n",
       "          -1.1367e-01,  1.2560e-01, -1.6956e-02, -2.8287e-01,  2.0633e-01,\n",
       "           3.1044e-01, -5.5060e-01,  3.4034e-01,  4.6079e-01,  3.3835e-02,\n",
       "           6.6069e-01],\n",
       "         [-1.2660e-01,  2.1726e-01, -2.3181e-01, -9.2214e-02, -1.2373e-01,\n",
       "          -8.4203e-02,  2.1314e-01,  1.9286e-01,  7.4409e-02,  9.4097e-02,\n",
       "           1.7761e-01, -5.0897e-01,  3.4032e-01,  3.7087e-01,  3.2967e-02,\n",
       "           6.7194e-01],\n",
       "         [-6.8336e-02,  2.4651e-01, -9.9911e-02, -1.4145e-01, -2.3869e-01,\n",
       "           4.1369e-02,  2.2146e-01,  6.4435e-02,  6.3163e-02,  8.6441e-02,\n",
       "           2.1065e-01, -5.7335e-01,  2.3020e-01,  4.1408e-01, -3.2978e-02,\n",
       "           6.0579e-01],\n",
       "         [ 3.0179e-03,  3.3763e-01, -1.9875e-01, -1.9649e-01, -2.5722e-01,\n",
       "           3.9542e-03,  3.1227e-01,  1.2246e-01,  8.9230e-02,  5.1725e-02,\n",
       "           1.2997e-01, -6.7409e-01,  3.1386e-01,  3.6818e-01, -7.1962e-02,\n",
       "           6.3033e-01]],\n",
       "\n",
       "        [[-2.2060e-02,  6.8009e-01, -8.9912e-02, -2.8198e-01, -4.5099e-01,\n",
       "          -5.7313e-01, -8.0043e-01,  5.7265e-01,  2.2719e-01, -2.2753e-01,\n",
       "          -3.3556e-02, -4.2345e-01,  2.4792e-01,  1.0445e-01,  1.8685e-01,\n",
       "          -1.2098e-01],\n",
       "         [ 2.4303e-01,  8.2499e-01, -4.9506e-01, -4.2666e-01, -4.1662e-01,\n",
       "          -4.2236e-01,  8.2038e-02,  5.6564e-01,  2.6884e-01, -2.1470e-01,\n",
       "          -2.4425e-01, -9.0251e-01,  5.8464e-01,  6.6889e-02, -7.9596e-02,\n",
       "           3.4368e-01],\n",
       "         [-1.0080e-01,  6.9447e-01, -4.1906e-01, -3.0635e-01,  2.6664e-02,\n",
       "          -2.0195e-01,  4.3612e-02,  3.3476e-01,  4.0676e-02,  3.4998e-02,\n",
       "           8.3621e-02, -6.9797e-01,  4.6349e-01,  4.7608e-01,  6.5331e-02,\n",
       "           5.2310e-01],\n",
       "         [ 5.2154e-02,  7.6367e-01, -5.4007e-01, -3.7293e-01, -7.5885e-02,\n",
       "          -2.1930e-01,  2.7530e-01,  3.9091e-01,  1.0839e-01, -2.4408e-02,\n",
       "          -5.1642e-02, -8.6982e-01,  5.7860e-01,  3.6399e-01, -3.8067e-02,\n",
       "           5.9501e-01],\n",
       "         [-4.0529e-03,  5.6689e-01, -4.3633e-01, -1.5729e-01, -9.7811e-02,\n",
       "          -1.6253e-01,  3.4815e-01,  5.6011e-01,  4.5733e-01, -1.1298e-01,\n",
       "          -1.3817e-01, -7.5531e-01,  5.2983e-01,  2.7491e-01, -2.4457e-02,\n",
       "           6.2085e-01],\n",
       "         [ 4.3627e-02,  5.4291e-01, -2.4919e-01, -2.0366e-01, -2.3655e-01,\n",
       "          -2.8766e-03,  3.3641e-01,  3.4968e-01,  3.8026e-01, -8.7705e-02,\n",
       "          -4.7366e-02, -7.8949e-01,  3.6919e-01,  3.4140e-01, -9.1887e-02,\n",
       "           5.5257e-01],\n",
       "         [ 1.0944e-01,  6.0454e-01, -3.4049e-01, -2.5769e-01, -2.5707e-01,\n",
       "          -3.9466e-02,  4.2459e-01,  3.7541e-01,  3.6614e-01, -1.0225e-01,\n",
       "          -1.0274e-01, -8.7413e-01,  4.4612e-01,  2.9959e-01, -1.2823e-01,\n",
       "           5.8846e-01],\n",
       "         [-4.2781e-03,  5.8182e-01, -3.3219e-01, -2.3149e-01, -1.0900e-01,\n",
       "          -5.9130e-03,  3.6716e-01,  3.1578e-01,  2.7132e-01, -2.3295e-02,\n",
       "           1.4951e-03, -7.9920e-01,  4.1898e-01,  4.2309e-01, -6.6596e-02,\n",
       "           6.2583e-01]],\n",
       "\n",
       "        [[ 3.8279e-01,  6.7785e-01, -7.9186e-01,  2.3051e-01, -1.4477e-01,\n",
       "          -4.6572e-01,  8.6314e-01, -3.5169e-01,  1.0868e-01,  7.4607e-01,\n",
       "           4.8688e-01, -3.7723e-01, -6.3321e-01, -3.9640e-01, -2.9958e-01,\n",
       "           5.5605e-01],\n",
       "         [ 4.1950e-01,  6.2996e-02, -2.9134e-01, -2.9708e-03, -2.2723e-02,\n",
       "          -7.9886e-02,  4.8366e-01, -1.9724e-01,  3.0614e-01,  5.1073e-01,\n",
       "           3.7030e-01, -5.3159e-01, -5.4538e-02, -2.1972e-01, -3.4979e-01,\n",
       "           5.5520e-01],\n",
       "         [ 2.2662e-01, -1.1445e-02, -5.2954e-01,  1.9029e-01,  1.8051e-01,\n",
       "           1.6972e-01,  6.3409e-01, -5.9142e-02,  8.8550e-02,  1.9649e-01,\n",
       "           4.8496e-01, -3.3821e-01, -1.3355e-01,  4.0330e-01,  7.2080e-02,\n",
       "           7.3251e-01],\n",
       "         [ 2.5379e-01, -1.7435e-01, -3.4582e-01,  3.0885e-01,  1.7410e-01,\n",
       "          -1.8943e-01,  5.0649e-01,  1.5601e-01,  4.1425e-02,  2.0466e-01,\n",
       "           4.9482e-01,  2.6753e-03, -3.2179e-01,  2.7550e-01,  2.3722e-01,\n",
       "           5.7422e-01],\n",
       "         [ 3.9412e-01,  2.3357e-02, -2.3109e-01,  1.1221e-01,  2.5878e-01,\n",
       "          -3.5165e-01,  5.7049e-01,  8.6377e-02, -6.2793e-02,  3.8445e-01,\n",
       "           5.1156e-01, -2.3771e-01, -9.7122e-02,  3.1632e-01,  1.6255e-01,\n",
       "           5.9229e-01],\n",
       "         [ 1.7843e-01,  8.7353e-02, -2.1279e-01, -6.6697e-02,  1.4824e-01,\n",
       "          -1.5953e-01,  4.4116e-01,  7.5271e-02, -1.1404e-01,  3.2731e-01,\n",
       "           5.1611e-01, -4.0770e-01, -1.8836e-02,  3.0022e-01, -5.8885e-02,\n",
       "           3.8975e-01],\n",
       "         [ 4.0069e-02,  1.3706e-01, -2.2023e-01, -6.7998e-02,  2.5773e-01,\n",
       "          -1.0195e-01,  3.7357e-01,  4.7136e-02, -1.5763e-01,  3.5610e-01,\n",
       "           5.4749e-01, -3.9233e-01,  1.7604e-02,  4.4348e-01, -8.1934e-05,\n",
       "           4.6007e-01],\n",
       "         [ 9.6032e-02,  2.8905e-01, -1.4962e-01, -1.7987e-01,  2.3317e-01,\n",
       "          -1.1817e-01,  3.4619e-01, -3.2944e-02, -1.0860e-02,  3.2029e-01,\n",
       "           4.7974e-01, -4.3124e-01,  1.2802e-02,  3.6389e-01, -3.1787e-02,\n",
       "           3.6579e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = MultiHeadAttention(2, 16)\n",
    "H = Head(16)\n",
    "\n",
    "H(xb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
