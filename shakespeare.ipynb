{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd()+'\\\\data\\\\input.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n",
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "#strategy to tokenize text\n",
    "\n",
    "#character level mapping\n",
    "\n",
    "s_to_i = {ch:i for i,ch in enumerate(chars)}\n",
    "i_to_s = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [s_to_i[c] for c in s]\n",
    "decode = lambda l: ''.join([i_to_s[c] for c in l])\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "#take all of the text and encode them into pytorch tensors\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into a train and validation split\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input the text sequences into the transformer\n",
    "\n",
    "#we only work with chunks of the dataset. when we train the model, we sample chunks of the dataset and train on just chunks at a time. chunks have a maximum length: called block_size\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "\n",
    "#shift to the right by 1 to get the target\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the input is tensor([18]), the target will be 47\n",
      "when the input is tensor([18, 47]), the target will be 56\n",
      "when the input is tensor([18, 47, 56]), the target will be 57\n",
      "when the input is tensor([18, 47, 56, 57]), the target will be 58\n",
      "when the input is tensor([18, 47, 56, 57, 58]), the target will be 1\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1]), the target will be 15\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15]), the target will be 47\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target will be 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"when the input is {context}, the target will be {target}\")\n",
    "\n",
    "#we want the model to be robust when seeing contexts of multiple lengths, we set it as 8 so that the model has the capability of looking back at least 'block_size' characters of contexts of. \n",
    "# When we need to predict the 9th or 10th character, we start to truncate the context to continue onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "\n",
    "batch_size = 64 #This is the value of B\n",
    "block_size = 256 #This is the value of T\n",
    "n_embd = 512 #This is the value of C\n",
    "n_head = 8\n",
    "dropout = 0.25\n",
    "n_layers = 8 #number of decoder blocks we will initialize\n",
    "max_new_tokens = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the batches\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y, = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# print('inputs:')\n",
    "# print(xb.shape)\n",
    "# print(xb)\n",
    "# print('targets:')\n",
    "# print(yb.shape)\n",
    "# print(yb)\n",
    "\n",
    "#xb, yb, of shape (B, T) where B = batch_size, and T = block_size (time sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer time!\n",
    "\n",
    "head -> multi-head self attention -> feedforward -> block -> Decoder\n",
    "\n",
    "special layers:\n",
    "dropout\n",
    "layernorm\n",
    "linear\n",
    "embedding\n",
    "pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch's positional encoding https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "            x: (T, B, C)\n",
    "            We have to change our shape dimensions in to (T, B, C) and then change it back to (B, T, C) when done\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"\n",
    "    One Head of self-attention \n",
    "    \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "        #initialize the key, query, and value matrices\n",
    "        self.Wk = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.Wq = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.Wv = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        #since this is a decoder, we need to initialize the mask as well\n",
    "\n",
    "        #we register this as a buffer, which still exists as a 'matrix' to use, but we don't compute gradients on this or use in the backward pass\n",
    "        #model parameters are objects that we use during the forward pass and we update using gradient descent\n",
    "        #model buffers are objects that we use during computation but do not update\n",
    "\n",
    "        #both parameters and buffers are saved to the right device when calling .to_device\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        K = self.Wk(x) #(B, T, head_size)\n",
    "        Q = self.Wq(x) #(B, T, head_size)\n",
    "        V = self.Wv(x) #(B, T, head_size)\n",
    "\n",
    "        #K.T needs to be of shape (B, C, T), so we swap the -2 and -1 positions\n",
    "        scores = Q @ K.transpose(-2, -1) * 1/(self.head_size)**2 #(B, T, T)\n",
    "        masked_scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B, T, T)\n",
    "        attention_scores = F.softmax(masked_scores, dim = -1) #applying softmax along the rows (B, T, T)\n",
    "        attention_scores = self.dropout(attention_scores) #(B, T, T)\n",
    "        out = attention_scores @ V #(B, T, head_size)\n",
    "        \n",
    "        return out #(B, T, head_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) #(B, T, n_heads*head_size)\n",
    "        self.proj = nn.Linear(n_heads * head_size, n_embd) #paper specifies a final linear layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input x is of size (B, T, C)\n",
    "\n",
    "        #Each Head returns a output of size (B, T, head_size), we concatenate along the final dimension so that our variable 'out' is now (B, T, n_heads*head_size)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) #(B, T, n_heads*head_size)\n",
    "        out = self.proj(out) #(B, T, C)\n",
    "        out = self.dropout(out) #(B, T, C)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #input and output will be of size (B, T, C)\n",
    "        self.ff1 = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.ff2 = nn.Linear(4*n_embd, n_embd)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume x is of shape (B, T, C)\n",
    "        x = self.ff1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.ff2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    #implementaion of one transformer block\n",
    "    def __init__(self, n_head):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embd // n_head\n",
    "\n",
    "        self.sa = MultiHeadAttention(n_head, self.head_size)\n",
    "        self.ffw = FeedForward()\n",
    "        self.layernorm1 = nn.LayerNorm(n_embd)\n",
    "        self.layernorm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #assume input of x is size (B, T, C) where x is the sum of the embedded input + positional_encoding\n",
    "        x = self.layernorm1(x) #(B, T, C)\n",
    "        x = x + self.sa(x) #(B, T, C)\n",
    "        x = self.layernorm2(x) #(B, T, C)\n",
    "        x = x + self.ffw(x) #(B, T, C)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embedding_matrix = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding = PositionalEncoding(n_embd)\n",
    "\n",
    "        # need '*' before list comprehension otherwise we get TypeError: list is not a Module subclass\n",
    "        self.blocks = nn.Sequential(*[Block(n_head) for _ in range(n_layers)])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, x, y = None):\n",
    "        B, T = x.shape\n",
    "        C = n_embd\n",
    "        #assume our inputs x are of size (B, T)\n",
    "        #assume our targets y are of size (B)\n",
    "\n",
    "        token_embed = self.tok_embedding_matrix(x) #(B, T, C)\n",
    "        pos_embed = self.pos_embedding(token_embed.view(T,B,C)).view(B, T, C) #(B, T, C)\n",
    "\n",
    "        input = token_embed + pos_embed #(B, T, C)\n",
    "        input = self.blocks(input) #(B, T, C)\n",
    "\n",
    "        input = self.final_layer_norm(input) #(B, T, C)\n",
    "        logits = self.final_linear(input) #(B, T, C)\n",
    "\n",
    "        if y is not None:\n",
    "            logits = logits.view(B*T, -1) #(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "    #generating some stuff\n",
    "    #idx is (B, T) array of indices in our current context <-- current context of some list of characters in some batch\n",
    "    #we keep extending (B, T) to (B, T+1), (B, T+2) and so on.. continuing until we reach max new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx_cond) #<-- output of this is (B, T, C)\n",
    "            #print(f\"new dim of logits: {logits.shape}\")\n",
    "            #focus only on the last time step because the last time step is the prediction on what comes next\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim =-1) # (B, C)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.274433 M parameters\n"
     ]
    }
   ],
   "source": [
    "#initializing the stuff\n",
    "\n",
    "model = Decoder()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (tok_embedding_matrix): Embedding(65, 512)\n",
      "  (pos_embedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (Wk): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wq): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (Wv): Linear(in_features=512, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (ffw): FeedForward(\n",
      "        (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (final_linear): Linear(in_features=512, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this function to estimate the loss every once in a while\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3642, val loss 4.3649\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OXETELY:\n",
      "Come;\n",
      "En caleshe neter, I'll betas Myon, witasharthine s yooncurtupit ithalflly s; he onis.\n",
      "Weth myonorenoue heably ds.'s winy let thalt th wit ine harkert-redy wing\n",
      "Withieat. Ifler ps y betho's burhou thes ond, ge movis ant thad toeyof th. usthis no I hour talold bonea unt tofo lt tag a sandend manigigives lot ves\n",
      "Myofe mby ay hitha ishim; s nelim uthis thene waty,\n",
      "ofethe ble plin hosome carifuks an. lits, of: inors I makestellve thor s thigremy thincof winey ieee t 'sonde chis of on haf wonsabe man the d, may athin y\n",
      "Ditoflpallo mit\n",
      "I'dreck rays,\n",
      "Be on My totheuneinont out!'s mof st this amermor t burdstucinous. cf humps, wrenden,\n",
      "Whis, ber flulowomblle men ding tof too uckempe foralt wit at weve, oforser in; our the thef,\n",
      "Yonevo Rearosthulthinrwhe bus tey t rexkinonoreceaby,\n",
      "Aroncrs iveng tuces tellongad an ofe at bus, withenely, trit han Leseg tupuponde s Talm this yonount! thon avigs wishires;\n",
      "Th'sout-pe\n",
      "Shan flinurin thoolve, honowie t atr w Ous seff t I fituswaltonchin \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(vocab_size, n_embd)\n\u001b[1;32m----> 3\u001b[0m xb \u001b[39m=\u001b[39m tokens(xb)\n\u001b[0;32m      5\u001b[0m xb\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\robots\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\robots\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\robots\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "tokens = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "xb = tokens(xb)\n",
    "\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.3046, -0.7895,  ..., -0.0000, -0.1914,  2.7128],\n",
       "         [-2.1587,  1.6099,  0.3175,  ..., -0.7726, -0.1732, -0.7560],\n",
       "         [-0.0228,  1.3423,  0.0000,  ...,  0.3276,  1.2187,  0.0000],\n",
       "         ...,\n",
       "         [ 1.1110,  1.8604, -0.6897,  ...,  2.7487,  0.0767,  2.5919],\n",
       "         [ 1.7454,  2.0296,  1.7677,  ...,  2.2489,  0.3629, -0.2505],\n",
       "         [-1.2237,  0.0000,  0.9099,  ..., -0.7726, -0.1730, -0.7560]],\n",
       "\n",
       "        [[ 1.5315, -1.4319,  2.7456,  ...,  0.3968,  1.0700,  0.2157],\n",
       "         [ 2.4284, -1.2822, -1.4963,  ...,  1.0317,  0.3879,  2.1038],\n",
       "         [ 1.6417,  1.3021,  0.1874,  ...,  3.0464, -0.9241,  2.8748],\n",
       "         ...,\n",
       "         [ 0.9672,  0.3293,  2.2789,  ...,  2.2489,  0.3633, -0.0000],\n",
       "         [-2.0836, -0.1690,  2.3959,  ...,  1.0396, -2.2987, -1.1044],\n",
       "         [ 0.0000, -0.8688,  1.8949,  ...,  0.3276,  1.2193,  2.0859]],\n",
       "\n",
       "        [[-2.0830, -1.2652,  2.4538,  ..., -0.0343,  0.4258,  2.5545],\n",
       "         [-0.8637, -0.4950,  1.6562,  ...,  0.3276,  0.0000,  2.0859],\n",
       "         [-0.6649,  0.5338, -0.4171,  ...,  2.7487,  0.0773,  2.5919],\n",
       "         ...,\n",
       "         [-3.3059,  1.2462,  1.6522,  ...,  0.0000, -2.2983, -0.0000],\n",
       "         [-1.0883,  0.5464,  1.1512,  ...,  0.3276,  1.2197,  2.0859],\n",
       "         [-0.8895,  1.5752, -0.9222,  ...,  2.7487,  0.0775,  2.5919]],\n",
       "\n",
       "        [[-0.7065, -0.0782, -1.4202,  ...,  1.0367, -0.5414, -0.0000],\n",
       "         [ 0.7009,  0.8586,  0.5473,  ...,  1.2402,  0.0000,  2.0274],\n",
       "         [-0.1026,  2.4387, -1.2370,  ..., -0.3970,  1.7834,  2.1337],\n",
       "         ...,\n",
       "         [ 0.7102, -0.5662,  0.1975,  ...,  0.0000, -1.1811,  0.0000],\n",
       "         [ 0.9060,  2.0977, -2.0751,  ...,  2.7487,  0.0779,  0.0000],\n",
       "         [ 1.4210,  0.0000,  0.5612,  ...,  1.4819,  0.4979, -0.2294]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = PositionalEncoding(32)\n",
    "B, T, C = xb.shape\n",
    "pos(xb.view(T,B,C)).view(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3696,  1.5744,  0.2509,  ..., -0.5898,  0.3345, -2.2403],\n",
       "         [ 0.7096, -2.3177, -0.5724,  ..., -0.5082, -0.8088,  0.2871],\n",
       "         [ 0.8979, -2.1066, -0.8966,  ..., -1.2316, -0.1825,  2.1817],\n",
       "         ...,\n",
       "         [-1.1446,  0.0414,  1.8208,  ..., -0.9161, -0.3360, -0.3574],\n",
       "         [ 0.4984, -1.2605, -0.7518,  ..., -0.9832, -1.1752,  0.1661],\n",
       "         [ 0.5135, -1.7658, -0.4623,  ..., -0.7278, -1.0931, -0.3946]],\n",
       "\n",
       "        [[ 0.1684, -1.6184,  1.0792,  ...,  0.9328, -0.1259,  1.9468],\n",
       "         [-0.1390,  2.1019,  1.9862,  ...,  1.2079,  0.1201,  1.2189],\n",
       "         [ 1.4138, -0.7045, -0.3073,  ..., -1.7178,  1.8023,  1.1076],\n",
       "         ...,\n",
       "         [ 0.2745, -1.1965, -1.2149,  ..., -0.8091, -0.5446,  0.0062],\n",
       "         [-0.5786,  1.1474, -0.6361,  ..., -0.4063,  0.2948,  0.0563],\n",
       "         [ 0.2589, -1.9941, -1.6683,  ..., -1.1319, -0.1778,  1.6774]],\n",
       "\n",
       "        [[ 1.0505,  1.2797,  0.4887,  ..., -1.5235, -0.5249, -0.4292],\n",
       "         [ 0.4774, -1.9272, -1.6452,  ..., -1.0834,  0.0637,  0.6691],\n",
       "         [-1.2309,  0.8857,  0.8584,  ..., -0.8521,  0.3718, -1.1956],\n",
       "         ...,\n",
       "         [-0.6139,  1.0338, -1.0152,  ..., -0.4090, -0.0162,  0.3854],\n",
       "         [ 0.7856, -1.9160, -1.7268,  ..., -1.0167,  0.0147,  1.5967],\n",
       "         [-0.8293,  0.7307,  1.0354,  ..., -1.5422,  0.0153, -0.2888]],\n",
       "\n",
       "        [[-0.2741, -0.2682,  2.2050,  ..., -0.6146,  2.1185, -0.4777],\n",
       "         [ 1.2813, -0.3069,  0.4019,  ..., -0.5620, -0.3132, -0.8304],\n",
       "         [-0.8668, -0.4545, -0.1923,  ...,  0.9333, -1.0915, -0.5831],\n",
       "         ...,\n",
       "         [-0.9535,  0.2801, -0.9778,  ..., -0.2235,  0.2428,  0.7869],\n",
       "         [-1.0084,  0.2673,  1.4210,  ..., -1.2814,  0.4389, -1.4179],\n",
       "         [ 0.7738,  0.8731, -0.9156,  ...,  0.3745,  2.1386, -0.0393]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#H = MultiHeadAttention(2, 16)\n",
    "#H = Head(16)\n",
    "H = Block(2)\n",
    "H(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
